{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "- powerful and versatile machine learning algorithms\n",
    "- capable of performing linear or non linear classification, regression, and even outlier detection.\n",
    "- well suited for classification of complex small and medium size dataset \n",
    "\n",
    "### Linear SVM Classification\n",
    "\n",
    "- The two classes can clearly be separated easily with a straight line (they are linearly separable).\n",
    "\n",
    "**Large Margin Classification**\n",
    "- SVM classifies two binary classed with maximum possible distance  \n",
    "- Dependent on the instance on the edge of the street \n",
    "- these edge instances are called support vectors\n",
    "- SVMs are sensitive to the feature scales\n",
    "- only works if data is linearly separable\n",
    "- sesnsitive to outliers\n",
    "\n",
    "**Soft Margin Classification**\n",
    "- modified version of the hard margin classification\n",
    "- can do it by passing the hyperparameter c \n",
    "    - high c - too soft, many boundary violations\n",
    "    - small c - too hard, strict boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "## Applying SVM on classification problem\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Understanding data\n",
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set has 4 features and 3 target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assigning X and Y\n",
    "## Only taking petal features and virginica target class\n",
    "X= iris[\"data\"][:,(2,3)]\n",
    "y= (iris[\"target\"] == 2).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating SVM Pipeline\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"linear_svc\",LinearSVC(C=1,loss=\"hinge\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting the pipeline of models and preprocessing to the feature set \n",
    "svm_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Predictive a random feature\n",
    "svm_clf.predict([[5.5,1.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot x and y and model\n",
    "y_pred = svm_clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x267f016a790>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X[:,1],X[:,0],\"b.\")\n",
    "plt.plot(X[:,1],y_pred,\"r.\")\n",
    "plt.plot(y_pred,X[:,0],\"g.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Linear kernal in SVC instead of using Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"scv_linear\",SVC(C=1,kernel=\"linear\")),\n",
    "])\n",
    "\n",
    "svm_clf.fit(X,y)\n",
    "print(svm_clf.predict([[5.5,1.7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SGDClassifier \n",
    "\n",
    "- in loss function the possible options are \n",
    "    - ‘hinge’,\n",
    "    - ‘log’, \n",
    "    - ‘modified_huber’, \n",
    "    - ‘squared_hinge’, \n",
    "    - ‘perceptron’,\n",
    "    - or a regression loss: \n",
    "        - ‘squared_loss’, \n",
    "        - ‘huber’, \n",
    "        - ‘epsilon_insensitive’, \n",
    "        - or ‘squared_epsilon_insensitive’.\n",
    "\n",
    "The ‘log’ loss gives logistic regression, a probabilistic classifier. ‘modified_huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates. ‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.\n",
    "\n",
    "- alpha : \n",
    "    - Constant that multiplies the regularization term. \n",
    "    - The higher the value, the stronger the regularization. \n",
    "    - used in calculation of learning rate\n",
    "- LR: \n",
    "    - ‘constant’: eta = eta0\n",
    "    - ‘optimal’: \n",
    "        - eta = 1.0 / (alpha * (t + t0)) \n",
    "        - where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "    - ‘invscaling’: eta = eta0 / pow(t, power_t)\n",
    "    - ‘adaptive’: eta = eta0,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "sgd_clf = Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"sgd_clf\",SGDClassifier(loss = \"hinge\",alpha=1/(1000*1))),\n",
    "])\n",
    "\n",
    "sgd_clf.fit(X,y)\n",
    "print(sgd_clf.predict([[5.5,1.7]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in book it's mentioned to use m and C for alpha, but it throws an error and it not part of documentation\n",
    "\n",
    "### Nonlinear SVM Classification \n",
    "\n",
    "- for non linearly separable datasets, including polynomial variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= make_moons(n_samples = 100, noise = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf_poly = Pipeline([\n",
    "    (\"poly_features\",PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"svm_clf\",LinearSVC(C=10,loss=\"hinge\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf_poly.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "\n",
    "- The kernel poly  makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_kernel_poly = Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"svm_clf\",SVC(kernel=\"poly\",degree=3,coef0=1,C=5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_kernel_poly.fit(X,y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features\n",
    "\n",
    "- to tackle the non linear problems \n",
    "- measures how each features resembles a particular landmark by puting values in the similarty function (eg: gaussian rbf) and comparing to landmark \n",
    "- creating landmark\n",
    "    - transforming each and every instance to a landmark and make it more probable to be a linearly separable dataset\n",
    "    - downside: \n",
    "        - increases the number of features\n",
    "        - computationally expensive\n",
    "        \n",
    "#### Gaussian RBF Kernel\n",
    "\n",
    "- usign the rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as C or Y increased it starts to overfit \n",
    "\n",
    "-------\n",
    "- Other kernels exist but are used much more rarely. \n",
    "- Some kernels are specialized for specific data structures. \n",
    "    - String kernels are sometimes used when classifying text documents or DNA sequences (e.g., using the string subsequence kernel or kernels based on the Levenshtein distance).\n",
    "    \n",
    "##### How to choose the kernel\n",
    "\n",
    "- start with linear kernel (if large dataset)\n",
    "    - SVC linear is fasters than SVC (kernel =linear)\n",
    "- start with rbf ( medium or small dataset)\n",
    "- experiment with a few other kernels, using cross-validation and grid search.\n",
    "\n",
    "### Computational Complexity\n",
    "\n",
    "- SVCLienar - O(m × n).\n",
    "- SVC: between O(m^2 × n) and O(m^3 ×n).\n",
    "- SGDClassifier: O(m × n)\n",
    "\n",
    "#### SVM Regression\n",
    "\n",
    "- reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations\n",
    "- The width of the street is controlled by a hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Polynomial regression\n",
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs can also be used for outlier detection; see Scikit-Learn’s documentation for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### Model Training\n",
    "\n",
    "#### Decision function and predictions\n",
    "\n",
    "The linear SVM classifier model predicts the class of a new instance x by simply computing the decision function w x + b = w x + ⋯ + w x + b. If the result is positive, the predicted class ŷ is the positive class (1), and otherwise it is the negative class (0); \n",
    "\n",
    "#### Training Objective\n",
    "\n",
    "Consider the slope of the decision function: it is equal to the norm of the weight vector, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal to ±1 are going to be twice as far away from the decision boundary. In other words, dividing the slope by 2 will multiply the margin by 2. The smaller the weight vector w, the larger the margin.\n",
    "\n",
    "So we want to minimize ∥ w ∥ to get a large margin. If we also want to void any margin violations (hard margin), then we need the decision function to be greater than 1 for all positive training instances and lower than –1 for negative training instances. If we define t=–1 for negative instances (if y = 0) and t = 1 for positive instances (if y = 1), then we can express this constraint as t (w x + b) ≥ 1 for all instances. We can therefore express the hard margin linear SVM classifier objective as the\n",
    "constrained optimization problem\n",
    "\n",
    "\n",
    "todo>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**1. What is the fundamental idea behind Support Vector Machines?**\n",
    "\n",
    "To define space for each cateogry based on the parameters, confining them to particular ranges/functions to predict and classify the target classed\n",
    "\n",
    "**2. What is a support vector?**\n",
    "\n",
    "Support vectors are the instances on based of which the space is divided  for dataset in order to classify their target class\n",
    "\n",
    "**3. Why is it important to scale the inputs when using SVMs?**\n",
    "\n",
    "if not scaled, the SVM may miss out the smaller features \n",
    "\n",
    "**4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?**\n",
    "\n",
    "distance from the boundary can be used as a confidence score (not directly used)\n",
    "probability, using log regression or predict_proba() function\n",
    "\n",
    "**5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?**\n",
    "\n",
    "- primal /dual only in linear svm, dual only in kernal \n",
    "- primal complexity prop to m, dual prop to m2 or m3 \n",
    "\n",
    "**6. Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease γ (gamma)? What about C?**\n",
    "\n",
    "Y /c increase either/both\n",
    "\n",
    "**7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?**\n",
    "\n",
    "/todo>\n",
    "\n",
    "**8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a\n",
    "SGDClassifier on the same dataset. See if you can get them to produce roughly\n",
    "the same model.**\n",
    "\n",
    "/tried above>\n",
    "\n",
    "**9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will need to use one-versus-the-rest to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version = 1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Train an SVM regressor on the California housing dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "housing_data = pd.read_csv(\"datasets/housing/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding another categorical variable to categorize income of households \n",
    "housing_data['income_cat'] = pd.cut(housing_data.median_income,\n",
    "                                    bins = [0,1.5,3.0,4.5,6,np.inf],\n",
    "                                    labels = [1,2,3,4,5])\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing_data,housing_data.income_cat):\n",
    "    strat_train_set = housing_data.loc[train_index]\n",
    "    strat_test_set = housing_data.loc[test_index]\n",
    "    \n",
    "\n",
    "    ## removing income cat\n",
    "strat_test_set.drop(\"income_cat\",axis =1,inplace = True )\n",
    "strat_train_set.drop(\"income_cat\",axis =1,inplace = True )\n",
    "\n",
    "    ## removing ocean proximity\n",
    "strat_test_set.drop(\"ocean_proximity\",axis =1,inplace = True )\n",
    "strat_train_set.drop(\"ocean_proximity\",axis =1,inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-ee5a8126ef11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msvm_housing_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrat_train_set\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrat_test_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm_housing_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrat_train_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    412\u001b[0m                              % self.C)\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse='csr',\n\u001b[0m\u001b[0;32m    415\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                                    accept_large_sparse=False)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    797\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m                           FutureWarning)\n\u001b[0;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    646\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     95\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     96\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     98\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "svm_housing_reg = Pipeline([\n",
    "    (\"scaler\",StandardScaler()),\n",
    "    (\"regressor\",LinearSVR(C=10)),\n",
    "    \n",
    "])\n",
    "\n",
    "svm_housing_reg.fit(strat_train_set,strat_test_set)\n",
    "y_pred = svm_housing_reg.predict(strat_train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
