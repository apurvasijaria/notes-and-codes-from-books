{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Networks are much more complex than the shallow NN trained in last chapter. Issues in trianing DNN:\n",
    "\n",
    "- vanishing graidents or exploding gradients\n",
    "- not enough training data\n",
    "- slow training\n",
    "- overfitting the trianing set\n",
    "\n",
    "\n",
    "### Vanishing and Exploding Gradients\n",
    "\n",
    "In backpropogation, the algo works by going from the output layer to the input layer and propogating the error gradient along the way. gradients often get small and smaller as the algorithm progressed down to the lower layers. resulting in no change in the lower layer leaved and training never converging to a good solution. this is called vanishing gradietns. \n",
    "\n",
    "In some cases, opposite happends, the gradients grow bigger and bigger until the layers get insanely large weight updates andthe algo diverges, this is called exploding gradients, which resurfaces in recurrent NN. \n",
    "\n",
    "MMore generaly, DNN suffer from unstable gradients, different layers may learn at widely different speeds.\n",
    "\n",
    "This was concluded in a 2010 study, that sigmoid function saturates at 0 or 1, hence the sgd has no gradient to propogate back from. \n",
    "\n",
    "__Glorot and He Initialization__\n",
    "\n",
    "for signifficantly allecviate the unstable gradient problem. we need to signal the flow properly in both directions: in the forward direction when making the predictions and in reverse direction when backpropogating gradients. we don't want the signal to die out or explode and saturate. for the signal to flow properly, we need the vairance of the outputs of each layer to be equal to the variance of its inputs and we need the graideitns tp have equal variance before and after flowing through a layer in reverse direction. it's not possible unless the layer has an equal number of input and neurons (fan in and fan out of the layer), but glorot and bengio proposed a compromize that is proven to work in ractive. the connection weights of each layer must be initialized randomly as ${fan}_{avg} = ({fan}_{in} + {fan}_{out})/2$.  this is called _xavier initialization or glorot initializtion_ \n",
    "\n",
    "\n",
    "| Initialization | Activation functions | σ² (Normal)|\n",
    "| :-: | :-: | :-: |\n",
    "| Glorot |  None, tanh, logistic, softmax | $ \\frac 1 {{fan}_{avg}}$|\n",
    "| He | ReLU and variants | $ \\frac 2 {{fan}_{in}}$ |\n",
    "| LeCun | SELU | $ \\frac 1 {{fan}_{in}}$|\n",
    "\n",
    "\n",
    "By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-ml-book",
   "language": "python",
   "name": "hands-on-ml-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
