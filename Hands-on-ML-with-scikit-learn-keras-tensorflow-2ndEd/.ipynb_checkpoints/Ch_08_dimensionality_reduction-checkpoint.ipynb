{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality\n",
    "\n",
    "ML problems often involves thousasnds or even million of features for each training instance. It make the trianin slow and also makes harder to find a good solutions. This is called curse of dimensionality, where the number of features cause hinderance in training a good model. \n",
    "\n",
    "Fortunately we can often reduce the number of dimension based on data, identifying the features which are not adding singnificant information to the model and drop them or combine two features to make a more meaningful feature. \n",
    "\n",
    "Less features can also help in generating deeper data insights from the important features. There are various techniques which can be used for feature reduction like PCA, Kernel PCA and LLE.\n",
    "\n",
    "In theory, one solution can be to increase the size of the training set to reach a sufficient density of training instances. In practice, this is hard to acchieve. The number of trianing instances required to reach a given density increases exponentially with the number of dimensions. \n",
    "\n",
    "\n",
    "### Main approaches of Dimensionality reduction\n",
    "\n",
    "There are two main approaches for reducing dimensionality:\n",
    "1. Projection\n",
    "2. Manifold Learning\n",
    "\n",
    "\n",
    "__Projection__\n",
    "\n",
    "\n",
    "In most practical problems, training instances are not uniformly spread out accross all dimensions. Some features many be constant or have high collinearity. Porjection involves projecting a image of the current dataset to a lower dimension and losing one degree of data. Ex. a 3D distribution of plot can be reduced to a 2D plot but losing either of x,y or z axis. However, since we are losing one axis of information it's might lead to loss in critical information eg: projecting a spiral (or alternatively swiss roll) graph on either axis with lose some critical information .\n",
    "\n",
    "\n",
    "__Manifold Learning__\n",
    "\n",
    "The swiss roll is an example of 2D manifold. 2D manifold is a 2D shape that can be bent and twisted in higher dimensional space. More generally a d dimensional manifold is part of a n dimensional manidfold with d<n that locally resembles a d dimnesional hyperplane. \n",
    "\n",
    "Many dimensionality reduction algorithms work on modeling the manifold on wihch the training instances lie, it's called manifold leanring. It replies on the manifold assumption (manifold hypothesis) which states that most real-world high dimensional datasets lie close to a much lower dimensional moanifold. This assumption is very often empirically observed. \n",
    "    \n",
    "The manifold assumption is often accompanied by another implicit assumption, that hte task at hand will be simpler if expressed in lower dimensional space of the manifold. This doens't hold always. \n",
    "\n",
    "In simple terms, reducing the dimensionality may lead to faster trianing but may not always increasr the model performance, it is very dataset depndent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Principal component analysis, first identifies the hyperplane that lies closest to the data and then it projects the data onto it.\n",
    "\n",
    "\n",
    "__Preserving the variance__\n",
    "\n",
    "First step is to choose the correct hyperplane. It is reasonable to select the axis that preserves the maximum amount of variance, as it will most likely presenrve more information than other projections. Another way to explain this choice is that it is the axis that minimises the mean squared distance between the original dataset and its projection into that axis.\n",
    "\n",
    "\n",
    "__Principal Components__\n",
    "\n",
    "PCA identifies the axis that has highest amount of vairance and also finds the second axis, orthogonal to the first one, that accounts for the largest amount of remaining vairance. The $i^{th}$ axis is called $i^{th}$  principal component of the data. We can find the principal components using a standard matrix factorization technique called SVD (Singular value decomposition) that can decompose the trianing matrix X into the matrix multiplication of the three matrices $U \\sum V^T $ where $V$ contains the unit vectors that define all the principal component that we are looking for.\n",
    "\n",
    "we can use ```numpy```'s ```svd()``` function to obtain the principal component of the trianing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.linspace(start=-1., stop=1., num=100)\n",
    "y = X + np.random.normal(size=100)/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.97979798 -0.95959596 -0.93939394 -0.91919192 -0.8989899\n",
      " -0.87878788 -0.85858586 -0.83838384 -0.81818182 -0.7979798  -0.77777778\n",
      " -0.75757576 -0.73737374 -0.71717172 -0.6969697  -0.67676768 -0.65656566\n",
      " -0.63636364 -0.61616162 -0.5959596  -0.57575758 -0.55555556 -0.53535354\n",
      " -0.51515152 -0.49494949 -0.47474747 -0.45454545 -0.43434343 -0.41414141\n",
      " -0.39393939 -0.37373737 -0.35353535 -0.33333333 -0.31313131 -0.29292929\n",
      " -0.27272727 -0.25252525 -0.23232323 -0.21212121 -0.19191919 -0.17171717\n",
      " -0.15151515 -0.13131313 -0.11111111 -0.09090909 -0.07070707 -0.05050505\n",
      " -0.03030303 -0.01010101  0.01010101  0.03030303  0.05050505  0.07070707\n",
      "  0.09090909  0.11111111  0.13131313  0.15151515  0.17171717  0.19191919\n",
      "  0.21212121  0.23232323  0.25252525  0.27272727  0.29292929  0.31313131\n",
      "  0.33333333  0.35353535  0.37373737  0.39393939  0.41414141  0.43434343\n",
      "  0.45454545  0.47474747  0.49494949  0.51515152  0.53535354  0.55555556\n",
      "  0.57575758  0.5959596   0.61616162  0.63636364  0.65656566  0.67676768\n",
      "  0.6969697   0.71717172  0.73737374  0.75757576  0.77777778  0.7979798\n",
      "  0.81818182  0.83838384  0.85858586  0.87878788  0.8989899   0.91919192\n",
      "  0.93939394  0.95959596  0.97979798  1.        ] [-0.82745727 -0.87735379 -0.70685062 -1.15255536 -0.88137317 -0.863486\n",
      " -0.82760977 -0.9805811  -0.75659769 -0.64457714 -0.93256428 -0.87763331\n",
      " -0.79396938 -0.63177833 -0.71420787 -0.68047772 -0.61043393 -0.52470962\n",
      " -0.55208607 -0.83833609 -0.57775221 -0.88233263 -0.56710075 -0.65349399\n",
      " -0.58260489 -0.44598111 -0.62188332 -0.51624489 -0.45703211 -0.35251268\n",
      " -0.43451334 -0.13820629 -0.49348789 -0.19016893 -0.28557894 -0.23630528\n",
      " -0.25622799 -0.28569959 -0.21866066 -0.21091609 -0.22522446 -0.11562536\n",
      " -0.22112717  0.20093183  0.09444166  0.15448171 -0.14882371 -0.24783385\n",
      " -0.02463707 -0.17282503  0.24637772 -0.06631169 -0.03491658  0.15167583\n",
      "  0.15595276  0.16168053  0.2386951   0.37982013  0.15511071  0.24830452\n",
      "  0.33588918  0.41910966  0.20247263  0.23567866  0.26034597  0.20414958\n",
      "  0.56330505  0.33675945  0.33934303  0.51309301  0.62170493  0.19074207\n",
      "  0.36998663  0.56509885  0.55089046  0.44260207  0.66357386  0.67657676\n",
      "  0.59287064  0.54375026  0.68840586  0.9678646   0.7387571   0.75941427\n",
      "  0.73570212  0.67059675  0.93191362  0.9813671   0.90249926  1.03682422\n",
      "  0.93064198  0.78759119  0.95810945  0.73948452  0.95904651  1.02166188\n",
      "  1.16850613  0.8162692   0.99116383  0.97602391]\n"
     ]
    }
   ],
   "source": [
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((X[..., None], y[..., None]), axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X  - X.mean(axis = 0)\n",
    "U, s, Vt= np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.15545496 -0.15576833 -0.1594272  ...  0.16042821  0.16735687\n",
      "   0.17002486]\n",
      " [-0.15810728 -0.09970137 -0.20380166 ...  0.16861109  0.04731634\n",
      "   0.07580685]\n",
      " [-0.14177949 -0.21702648  0.94678287 ...  0.04733845  0.02747944\n",
      "   0.03264247]\n",
      " ...\n",
      " [ 0.14562432  0.18190214  0.04719784 ...  0.95750536 -0.02670386\n",
      "  -0.03093181]\n",
      " [ 0.16233029  0.06112221  0.02683705 ... -0.02620504  0.97548546\n",
      "  -0.02551416]\n",
      " [ 0.1626711   0.08984558  0.03209677 ... -0.03053264 -0.02562641\n",
      "   0.97262224]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.43848281 0.87319937]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6869603   0.72669495]\n",
      " [ 0.72669495 -0.6869603 ]]\n"
     ]
    }
   ],
   "source": [
    "print(Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6869603  0.72669495]\n"
     ]
    }
   ],
   "source": [
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.72669495 -0.6869603 ]\n"
     ]
    }
   ],
   "source": [
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA assums that hte dataset is centered around the origin. Hence we need to preprocess the data if it's not, before applying PCA to it.\n",
    "\n",
    "\n",
    "### Projecting Down to d dimensions\n",
    "\n",
    "Once we have all the principal components, then we can reduce the dimnesionality of the dataset down to d dimension by projecting. Selecting this hyperplane with ensure that we preserve as much vairance as possible. \n",
    "\n",
    "To project we compute the matrix multiplication of the training set X and Wd (contains the first d columns of V).\n",
    "\n",
    "$X _{d-proj} = X W_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:,:2] ##Projecting on 2D \n",
    "X2D = X_centered.dot(W2)\n",
    "#print(X2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using sklearn for PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D_sk = pca.fit_transform(X)\n",
    "#n_components holds the Wd matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained Vairance Ratio\n",
    "\n",
    "Another useful piece of information is the explained variance ratio of each\n",
    "principal component, available via the __explained_variance_ratio___\n",
    "variable. The ratio indicates the proportion of the dataset’s variance that lies along each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98940569, 0.01059431])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_\n",
    "##98.9% of dataset vairance lie in 1st PC, and 1% lies on 2nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the right #dimensions\n",
    "\n",
    "Instead of randomly choosing the number of reduced dimensions, we can see where the 95% of variance lies and then select that dimension count.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98940569, 1.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can select n_components = d or specify a ratio between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=d)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both above generate same result. Another option is to plot the explained vairance as a function od the number of dimensions. There will an elbow in the curve, where the explained variance stops growing fast. This way we can see how many dimenions do not contribute significantly to the variance and churn them out. \n",
    "\n",
    "\n",
    "#### PCA for Compression\n",
    "\n",
    "After reducing the dimensions, training time is significantly reduced. This also called compression, as we are trying to compress maximum information of the model in lesser features. It's possible to decompress the model by inverse transformation of the PCA projection, this won't give back the original data since the projection loses some infomation but will be closer to the original data. The mean squared distance between the original data and the reconstructed data is called the reconstruction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Applying PCA in MNIST dtabase\n",
    "\n",
    "##Loading the data\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version = 1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = mnist[\"data\"],mnist[\"target\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating test and train set\n",
    "## MNIST already divided into train and test (first 60k train, next 10k test)\n",
    "\n",
    "##CAST to integer\n",
    "y = y.astype(np.uint8)\n",
    "X_train, X_test, y_train, y_test = X[:60000],X[60000:],y[:60000],y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X2D_sk = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09704664 0.16800588 0.22969677 0.28359097 0.33227894 0.37540125\n",
      " 0.40812055 0.4369595  0.4645798  0.4881498  0.5092417  0.52947161\n",
      " 0.54662979 0.56355091 0.57933732 0.59416685 0.60741246 0.62018143\n",
      " 0.63205406 0.6435809  0.65424256 0.66430969 0.67384542 0.68297086\n",
      " 0.69180491 0.7001981  0.70832389 0.71618755 0.72363488 0.73054347\n",
      " 0.73712441 0.74360589 0.74963204 0.75549786 0.76119807 0.76663435\n",
      " 0.77169222 0.7765708  0.7813851  0.78610776 0.79067523 0.79512359\n",
      " 0.7993086  0.80329076 0.8071405  0.81089154 0.81451162 0.81802754\n",
      " 0.82142812 0.82464686 0.82783703 0.83096508 0.83392491 0.83681446\n",
      " 0.83965576 0.84237012 0.84506533 0.84765006 0.85018776 0.85263557\n",
      " 0.85504063 0.85743326 0.85973735 0.86195267 0.86408988 0.86616213\n",
      " 0.86819256 0.87016038 0.87208891 0.87397523 0.875845   0.87765583\n",
      " 0.87943146 0.88118044 0.88283802 0.88447696 0.88609158 0.88764274\n",
      " 0.88911887 0.89055063 0.89197158 0.89338311 0.89478485 0.89614222\n",
      " 0.89748069 0.89880465 0.90010622 0.90136495 0.90259324 0.90380908\n",
      " 0.90497942 0.90612816 0.9072606  0.90836946 0.90945948 0.91052871\n",
      " 0.91157067 0.91261074 0.9136233  0.91462857 0.91561259 0.91656228\n",
      " 0.91750362 0.91841978 0.91932763 0.92022451 0.9210899  0.92194507\n",
      " 0.92279069 0.92361319 0.92440477 0.92519071 0.92597531 0.92674415\n",
      " 0.92750816 0.92826125 0.92899803 0.92972517 0.93044482 0.93115163\n",
      " 0.93184706 0.93253922 0.93322252 0.93389658 0.93456346 0.93520872\n",
      " 0.93584432 0.93647597 0.9370989  0.93770419 0.93830778 0.93890226\n",
      " 0.93949057 0.94007709 0.94065843 0.94123526 0.94180063 0.94235539\n",
      " 0.94289056 0.94341649 0.94394158 0.94445184 0.94495481 0.94545589\n",
      " 0.9459546  0.94644567 0.94693121 0.94741405 0.94788807 0.94835642\n",
      " 0.94882302 0.94928635 0.94974564 0.95019602 0.95064483 0.95108754\n",
      " 0.95152421 0.95195166 0.95237664 0.95279865 0.95321416 0.9536238\n",
      " 0.95402463 0.95442251 0.95481659 0.95520708 0.95559293 0.95597271\n",
      " 0.95635161 0.95672555 0.95709496 0.95746011 0.95782393 0.9581838\n",
      " 0.95853845 0.95889134 0.95923863 0.95958459 0.95992604 0.96026368\n",
      " 0.96060078 0.960931   0.96125936 0.96158526 0.96190821 0.96222948\n",
      " 0.96254678 0.96286295 0.9631739  0.96348343 0.9637894  0.96409267\n",
      " 0.96439508 0.96469665 0.96499336 0.96528911 0.96558258 0.96587525\n",
      " 0.966163   0.96644735 0.96672931 0.96700838 0.96728412 0.96755659\n",
      " 0.96782603 0.96809374 0.96835886 0.96862318 0.96888507 0.96914452\n",
      " 0.96940307 0.96966    0.96991473 0.97016755 0.97041984 0.9706694\n",
      " 0.97091792 0.97116399 0.97140794 0.97164981 0.97189078 0.97213122\n",
      " 0.97236949 0.97260634 0.97283892 0.97307023 0.97330036 0.97352845\n",
      " 0.97375603 0.9739788  0.97420067 0.97442111 0.97463898 0.97485563\n",
      " 0.9750708  0.97528499 0.97549785 0.97570822 0.97591797 0.97612494\n",
      " 0.97632966 0.97653393 0.97673525 0.97693623 0.97713523 0.97733286\n",
      " 0.97752868 0.97772241 0.97791527 0.97810704 0.97829859 0.97848766\n",
      " 0.97867568 0.97886207 0.97904792 0.97923253 0.97941623 0.97959938\n",
      " 0.97978126 0.97996112 0.98013868 0.98031537 0.98049133 0.98066521\n",
      " 0.98083864 0.98101129 0.98118267 0.98135271 0.98152201 0.98169001\n",
      " 0.98185694 0.98202339 0.98218759 0.98235108 0.98251373 0.98267562\n",
      " 0.98283652 0.98299636 0.98315519 0.98331254 0.98346928 0.98362418\n",
      " 0.98377799 0.98393106 0.9840831  0.98423311 0.98438277 0.984531\n",
      " 0.98467867 0.9848248  0.98496945 0.98511269 0.98525541 0.98539748\n",
      " 0.98553923 0.98567972 0.98581796 0.98595593 0.98609277 0.98622875\n",
      " 0.98636454 0.98649905 0.98663267 0.98676501 0.98689571 0.9870259\n",
      " 0.98715507 0.98728318 0.98741069 0.98753791 0.98766425 0.98779011\n",
      " 0.98791484 0.9880384  0.98816102 0.98828259 0.98840362 0.98852359\n",
      " 0.98864301 0.98876196 0.98887983 0.98899689 0.98911338 0.98922948\n",
      " 0.98934401 0.98945823 0.98957064 0.98968191 0.98979245 0.98990288\n",
      " 0.99001294 0.99012192 0.99023024 0.99033784 0.99044484 0.99055105\n",
      " 0.99065697 0.99076202 0.99086544 0.9909677  0.99106974 0.99117059\n",
      " 0.99127085 0.99137045 0.99146982 0.99156795 0.99166554 0.99176237\n",
      " 0.9918579  0.99195244 0.99204624 0.99213996 0.99223246 0.99232404\n",
      " 0.99241544 0.99250612 0.99259619 0.99268556 0.99277472 0.99286298\n",
      " 0.99295031 0.99303662 0.99312219 0.99320735 0.99329195 0.99337526\n",
      " 0.99345824 0.99354074 0.99362265 0.99370374 0.99378465 0.99386425\n",
      " 0.99394372 0.99402235 0.99410041 0.99417783 0.99425443 0.99433023\n",
      " 0.99440484 0.99447868 0.99455225 0.99462507 0.99469778 0.99476985\n",
      " 0.99484127 0.99491207 0.99498172 0.99505097 0.99511927 0.99518721\n",
      " 0.99525427 0.9953205  0.99538574 0.99544994 0.99551366 0.99557711\n",
      " 0.99563914 0.99570043 0.99576075 0.99582068 0.99588004 0.99593877\n",
      " 0.99599739 0.99605478 0.99611151 0.99616788 0.9962235  0.9962784\n",
      " 0.99633314 0.99638761 0.99644094 0.99649369 0.99654624 0.99659844\n",
      " 0.99665029 0.99670131 0.99675169 0.99680144 0.99685051 0.99689921\n",
      " 0.99694715 0.99699436 0.99704102 0.99708747 0.99713307 0.99717798\n",
      " 0.99722169 0.99726462 0.99730723 0.99734961 0.99739179 0.99743357\n",
      " 0.99747496 0.9975158  0.99755609 0.99759468 0.99763293 0.99767107\n",
      " 0.99770896 0.99774628 0.99778336 0.99781995 0.99785571 0.99789107\n",
      " 0.99792632 0.99796119 0.99799574 0.99803013 0.99806353 0.99809653\n",
      " 0.99812873 0.99816067 0.99819231 0.99822332 0.99825373 0.99828388\n",
      " 0.99831379 0.998343   0.99837187 0.99840039 0.99842845 0.99845636\n",
      " 0.99848388 0.99851082 0.9985366  0.99856189 0.99858705 0.99861188\n",
      " 0.99863657 0.99866092 0.99868524 0.99870949 0.99873321 0.99875656\n",
      " 0.99877978 0.9988023  0.99882466 0.99884678 0.99886822 0.99888951\n",
      " 0.99891058 0.99893143 0.99895181 0.9989719  0.99899177 0.99901139\n",
      " 0.9990309  0.99904986 0.99906831 0.9990865  0.9991046  0.99912256\n",
      " 0.99914015 0.99915756 0.99917392 0.99919009 0.9992062  0.9992221\n",
      " 0.99923764 0.99925285 0.99926801 0.99928299 0.99929775 0.99931233\n",
      " 0.99932669 0.99934086 0.99935482 0.99936861 0.99938229 0.99939562\n",
      " 0.99940873 0.99942127 0.99943366 0.99944584 0.99945784 0.99946976\n",
      " 0.99948153 0.99949326 0.99950464 0.99951596 0.99952721 0.99953821\n",
      " 0.99954914 0.99955971 0.99957025 0.99958049 0.99959051 0.99960008\n",
      " 0.99960945 0.99961871 0.99962794 0.99963681 0.99964562 0.99965421\n",
      " 0.99966237 0.99967034 0.99967821 0.99968594 0.99969359 0.99970116\n",
      " 0.9997087  0.99971606 0.99972333 0.99973048 0.99973744 0.99974432\n",
      " 0.99975104 0.9997577  0.99976423 0.9997707  0.99977684 0.99978277\n",
      " 0.9997885  0.99979418 0.99979973 0.99980508 0.99981035 0.99981553\n",
      " 0.99982057 0.99982557 0.9998305  0.9998354  0.99984021 0.99984491\n",
      " 0.99984953 0.99985409 0.99985828 0.99986247 0.99986648 0.99987048\n",
      " 0.99987443 0.99987823 0.99988201 0.99988532 0.99988856 0.99989177\n",
      " 0.99989495 0.99989807 0.99990115 0.99990417 0.99990719 0.99991016\n",
      " 0.99991311 0.99991604 0.99991884 0.99992147 0.99992402 0.99992638\n",
      " 0.9999287  0.99993096 0.99993321 0.99993541 0.99993756 0.99993971\n",
      " 0.99994181 0.99994385 0.99994573 0.9999476  0.99994945 0.9999513\n",
      " 0.9999531  0.99995485 0.99995647 0.99995807 0.99995958 0.99996104\n",
      " 0.99996245 0.99996384 0.99996516 0.99996646 0.99996775 0.99996899\n",
      " 0.99997011 0.99997119 0.99997224 0.99997328 0.99997432 0.9999753\n",
      " 0.99997626 0.9999772  0.99997811 0.99997902 0.99997992 0.9999808\n",
      " 0.99998163 0.99998241 0.99998315 0.99998387 0.99998454 0.9999852\n",
      " 0.99998584 0.99998646 0.99998709 0.99998768 0.99998826 0.99998883\n",
      " 0.99998939 0.99998992 0.99999044 0.99999092 0.99999139 0.99999186\n",
      " 0.99999229 0.99999271 0.99999311 0.9999935  0.99999386 0.99999422\n",
      " 0.99999453 0.99999483 0.99999511 0.99999538 0.99999564 0.99999589\n",
      " 0.99999613 0.99999637 0.9999966  0.99999682 0.99999704 0.99999724\n",
      " 0.99999744 0.99999764 0.99999782 0.99999799 0.99999814 0.99999829\n",
      " 0.99999842 0.99999856 0.99999869 0.99999879 0.99999889 0.99999898\n",
      " 0.99999906 0.99999914 0.99999921 0.99999928 0.99999934 0.9999994\n",
      " 0.99999946 0.9999995  0.99999955 0.99999959 0.99999963 0.99999966\n",
      " 0.9999997  0.99999973 0.99999976 0.99999979 0.99999982 0.99999985\n",
      " 0.99999987 0.99999988 0.9999999  0.99999992 0.99999993 0.99999994\n",
      " 0.99999995 0.99999996 0.99999996 0.99999997 0.99999998 0.99999998\n",
      " 0.99999999 0.99999999 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=d)\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_reduced,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_clf.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9832"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/svm_clf_pca.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(svm_clf, 'models/ch_08/svm_clf_pca.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy when we used all the 784 features  was aroung 95% (in ch3) with 154 feautures is 98%, which is better with a reduced training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Inverse PCA\n",
    "X_train_recovered = pca.inverse_transform(X_train_reduced) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_recovered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/pca.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pca, 'models/ch_08/pca.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized PCA\n",
    "\n",
    "sklearn can select the first d components randomly on setting the svd_solver = randomized in sklearn. It's computation complexity is reduced significantly and it's dramatically faster then full svd with d is much smaller than n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components = d, svd_solver = 'randomized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = rnd_pca.fit_transform(X_train)\n",
    "X_test_reduced = rnd_pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_reduced,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svm_clf.predict(X_test_reduced)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/svm_clf_pca_rnd.pkl']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(svm_clf, 'models/ch_08/svm_clf_pca_rnd.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/rnd_pca.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(rnd_pca, 'models/ch_08/rnd_pca.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By defualt, the csv_solver is set to auto, if we want to use full svd we can use svd_solver = 'full'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental PCA\n",
    "\n",
    "One issue with PCA is that they require the whole trianing set to fit in memory in order for the algorithm to run. Incremetnal PCA allow to split the data into mini batches and feed one batch at a time. It's good for large trianing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Spliting mnist dataset into 100 mini btaches\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components = d)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = inc_pca.transform(X_train)\n",
    "X_test_reduced = inc_pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/svm_clf_pca_inc.pkl']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_reduced,y_train)\n",
    "y_pred = svm_clf.predict(X_test_reduced)\n",
    "accuracy_score(y_pred, y_test)\n",
    "dump(svm_clf, 'models/ch_08/svm_clf_pca_inc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/inc_pca.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(inc_pca, 'models/ch_08/inc_pca.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "\n",
    "Kernel PCA uses the same trick as SVM's kernel trick, which uses kernel to fit a non linear model instead of transformign the non linear data to linear.  Kernel PCA is good at preserving clusters of instances after projections or unrolling the datasets that are in twisted manifolds.\n",
    "\n",
    "```sklean``` as a KernelPCA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X,y= make_moons(n_samples = 100, noise = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel = 'rbf', gamma = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a kernel and tuning hyperparamters\n",
    "\n",
    "kPCA is an unsupervised learning algorithm, there's no obvious performance measure to help us select the best kernel and hyperparamter values. Dimensionality reduction is often a preparation step for a supervised learning task. Hence, we can use grid search for selecting the kernel and hyperparameter that lead to the best performance on that task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "        (\"kpca\",KernelPCA(n_components = 2)),\n",
    "        (\"log_reg\",LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "        \"kpca__gamma\": np.linspace(0.03,0.05,10),\n",
    "        \"kpca__kernel\": [\"rbf\",\"sigmoid\"]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(clf, param_grid , cv= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),\n",
       "                                       ('log_reg', LogisticRegression())]),\n",
       "             param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          'kpca__kernel': ['rbf', 'sigmoid']}])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/kpca.pkl']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(grid_search,'models/ch_08/kpca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.03, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach, entirely unsupervised is to select the kernel and hyperparameters with lowest reconstruction error.  \n",
    "\n",
    "If we could invert the linear PCA step for a given instance in the reduced space, the reconstructed point would lie in feature space, not in the\n",
    "original space. Since the feature space is infinite-dimensional, we cannot compute the reconstructed point, and therefore we cannot compute the true\n",
    "reconstruction error. Fortunately, it is possible to find a point in the original space that would map close to the reconstructed point. This point is called the __reconstruction pre-image__. Once we have this pre-image, we can\n",
    "measure its squared distance to the original instance. You we then select the\n",
    "kernel and hyperparameters that minimize this reconstruction pre-image\n",
    "error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35143749336482416"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Defining the kPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.03, fit_inverse_transform=True)\n",
    "\n",
    "##Processing the data\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "##Calculating the recontruction error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4969280994317924"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Defining the kPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.05, fit_inverse_transform=True)\n",
    "\n",
    "##Processing the data\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "\n",
    "##Calculating the recontruction error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search paramters have lower reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE \n",
    "\n",
    "Locally linear embedding or LLE is a powerful nonlinear dimensionality reduction technique. It's a manifold learning technique that doesn't reply on projections. It works by first measuring how each trianing instance linearly related to the closest neighboards and then looking for a lower dimnesional representation of the trianing set where these local relationships are best preserved/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components = 2, n_neighbors = 10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_08/lle.pkl']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(lle,'models/ch_08/lle.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting 2D dataset is completely unrolled with distance between instance locally well preserved. The distances are not however, preserved on a larger scale. It nevertheless, does a good job at modeling the manifold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LLE Workings__\n",
    "\n",
    "1. for each training instance $x^i$ the algorithm identifies its k closest neighbors\n",
    "2. it then tried to reconstruct $x^i$ as a linear function of these neighbors. It finds the weights such that the squared distance between xi and the weighted sum of m instance is as small as possible. Assuming the w =0 if xj is not one of the k closest neighbors.\n",
    "3. the weight matrix is computed, that contains the wights for all instances\n",
    "4. the second constraint normalizes the weight of all training instances xi\n",
    "5. Next step is to map the training instances into a d-dimensional space while preserving the local relationships as much as possible. \n",
    "6. this leads to an unconstrained optimization problem \n",
    "7. we keep the weights fixed and find the optimal position of hte instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other techniques\n",
    "\n",
    "\n",
    "Some other popular dimensionality reduction techniques are:\n",
    "\n",
    "__Random Projections__\n",
    "\n",
    "This projects the data to a lower dimension using random linear projections. The random projects preserve the distances very well. The quality of the dimensionality reduction depends on the number of instances and the target dimensionality, but surprisingly not on the initial dimensionality. Can be implemented using ```sklearn.random_projection```\n",
    "\n",
    "__Multidimensional Scaling (MDS)__\n",
    "\n",
    "Reduces dimensionality while trying to preserve the distances betweenthe instances\n",
    "\n",
    "__Isomap__\n",
    "\n",
    "Creates a graph by connecting each instances to its nearest neighbors, then reduces dimensionality while trying to preserve the _geodesic distances_ btw the instances\n",
    "\n",
    "__t-distributed Stochastic Neighbor Embedding (t-SNE)__\n",
    "\n",
    "tries to keep the similar instances close and dissimilar instnaces apart. It's mostly used for visualization in particular to visualize the clusters of instances in high dimensional space.\n",
    "\n",
    "__Linear Discriminant Analysis (LDA)__\n",
    "\n",
    "Is a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit of this approach is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "__1. What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?__\n",
    "\n",
    "\n",
    "__2. What is the curse of dimensionality?__\n",
    "\n",
    "\n",
    "__3. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?__\n",
    "\n",
    "\n",
    "__4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?__\n",
    "\n",
    "\n",
    "__5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?__\n",
    "\n",
    "\n",
    "__6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?__\n",
    "\n",
    "\n",
    "__7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?__\n",
    "\n",
    "\n",
    "__8. Does it make any sense to chain two different dimensionality reduction algorithms?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-ml-book",
   "language": "python",
   "name": "hands-on-ml-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
