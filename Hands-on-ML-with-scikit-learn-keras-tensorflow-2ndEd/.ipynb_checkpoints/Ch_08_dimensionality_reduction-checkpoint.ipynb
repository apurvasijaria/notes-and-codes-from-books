{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality\n",
    "\n",
    "ML problems often involves thousasnds or even million of features for each training instance. It make the trianin slow and also makes harder to find a good solutions. This is called curse of dimensionality, where the number of features cause hinderance in training a good model. \n",
    "\n",
    "Fortunately we can often reduce the number of dimension based on data, identifying the features which are not adding singnificant information to the model and drop them or combine two features to make a more meaningful feature. \n",
    "\n",
    "Less features can also help in generating deeper data insights from the important features. There are various techniques which can be used for feature reduction like PCA, Kernel PCA and LLE.\n",
    "\n",
    "In theory, one solution can be to increase the size of the training set to reach a sufficient density of training instances. In practice, this is hard to acchieve. The number of trianing instances required to reach a given density increases exponentially with the number of dimensions. \n",
    "\n",
    "\n",
    "### Main approaches of Dimensionality reduction\n",
    "\n",
    "There are two main approaches for reducing dimensionality:\n",
    "1. Projection\n",
    "2. Manifold Learning\n",
    "\n",
    "\n",
    "__Projection__\n",
    "\n",
    "\n",
    "In most practical problems, training instances are not uniformly spread out accross all dimensions. Some features many be constant or have high collinearity. Porjection involves projecting a image of the current dataset to a lower dimension and losing one degree of data. Ex. a 3D distribution of plot can be reduced to a 2D plot but losing either of x,y or z axis. However, since we are losing one axis of information it's might lead to loss in critical information eg: projecting a spiral (or alternatively swiss roll) graph on either axis with lose some critical information .\n",
    "\n",
    "\n",
    "__Manifold Learning__\n",
    "\n",
    "The swiss roll is an example of 2D manifold. 2D manifold is a 2D shape that can be bent and twisted in higher dimensional space. More generally a d dimensional manifold is part of a n dimensional manidfold with d<n that locally resembles a d dimnesional hyperplane. \n",
    "\n",
    "Many dimensionality reduction algorithms work on modeling the manifold on wihch the training instances lie, it's called manifold leanring. It replies on the manifold assumption (manifold hypothesis) which states that most real-world high dimensional datasets lie close to a much lower dimensional moanifold. This assumption is very often empirically observed. \n",
    "    \n",
    "The manifold assumption is often accompanied by another implicit assumption, that hte task at hand will be simpler if expressed in lower dimensional space of the manifold. This doens't hold always. \n",
    "\n",
    "In simple terms, reducing the dimensionality may lead to faster trianing but may not always increasr the model performance, it is very dataset depndent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Principal component analysis, first identifies the hyperplane that lies closest to the data and then it projects the data onto it.\n",
    "\n",
    "\n",
    "__Preserving the variance__\n",
    "\n",
    "First step is to choose the correct hyperplane. It is reasonable to select the axis that preserves the maximum amount of variance, as it will most likely presenrve more information than other projections. Another way to explain this choice is that it is the axis that minimises the mean squared distance between the original dataset and its projection into that axis.\n",
    "\n",
    "\n",
    "__Principal Components__\n",
    "\n",
    "PCA identifies the axis that has highest amount of vairance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-ml-book",
   "language": "python",
   "name": "hands-on-ml-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
