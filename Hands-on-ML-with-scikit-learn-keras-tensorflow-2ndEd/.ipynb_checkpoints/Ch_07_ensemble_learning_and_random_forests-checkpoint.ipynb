{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "__Ensemble__\n",
    "\n",
    "A group of predictors is called an ensemble, the technique of grouping the predictors is called ensemble learning, and ensemble learning algorithm is called an ensemble method. \n",
    "\n",
    "Ex. random forest, a group of decision trees is an ensemble method.\n",
    "\n",
    "Different algorithms can be ensembled in different ways, like stacking, boosting and bagging. \n",
    "\n",
    "\n",
    "### Voting Classifiers\n",
    "\n",
    "We can combine different classifiers and select the most predicted class from the result (mode of predictors). Such combining is called voting classifier with hard voting, since the prediction class with most votes from the different classifier is the final classified class. Often, the voting classifier has higher accuracy than individual distinct classifiers. \n",
    "\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy\n",
    "\n",
    "Creating the voting classifier in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "            estimators = [('lr',log_clf),('rf',rf_clf),('svc',svm_clf)],\n",
    "            voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples = 10000, noise = 0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.878\n",
      "RandomForestClassifier 0.989\n",
      "SVC 0.9893333333333333\n",
      "VotingClassifier 0.988\n"
     ]
    }
   ],
   "source": [
    "## Checking accuracy for all the classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifiers = [log_clf, rf_clf, svm_clf, voting_clf]\n",
    "\n",
    "for clf in classifiers: \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    dump(clf,''.join(['models/ch_07/',clf.__class__.__name__,'.pkl']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier performs similar to the better classifier. \n",
    "\n",
    "__Soft Voting__\n",
    "\n",
    "If the classifiers have a ```predict_proba()``` class then sklearn can predict class with highest probability, this is called soft voting. It's often better than hard voting because it gives higher weight to high confidence votes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability = True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "            estimators = [('lr',log_clf),('rf',rf_clf),('svc',svm_clf)],\n",
    "            voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.878\n",
      "RandomForestClassifier 0.9873333333333333\n",
      "SVC 0.9893333333333333\n",
      "VotingClassifier 0.9876666666666667\n"
     ]
    }
   ],
   "source": [
    "## Checking accuracy for all the classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifiers = [log_clf, rf_clf, svm_clf, voting_clf]\n",
    "\n",
    "for clf in classifiers: \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    dump(clf,''.join(['models/ch_07/soft_voting_',clf.__class__.__name__,'.pkl']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "Another method of ensemble is where we train the same algorithm on different subset of the training set. When sampling is performed with replacement, this is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it's called pasting. both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias\n",
    "and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging and pasting in sklearn\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(),\n",
    "        n_estimators = 500,\n",
    "        max_samples = 100,\n",
    "        bootstrap = True,\n",
    "        n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier 0.978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/bag_clf.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "dump(bag_clf,'models/ch_07/bag_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with Decision Tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/dt_clf.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train[:100],y_train[:100])\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "print(dt_clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "dump(dt_clf,'models/ch_07/dt_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Bag Evaluation\n",
    "\n",
    "In bagging, some samples may be sampples several times and some might not be sampled at all. By default, BaggingClassifier samples m trianing instances with replacement where m is the size of the trianing set. Hence some instances are kept oob evaluation, on which trianing is not done. We can evaluate the ensemble by averaging the oob evaluations of each predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing oob evaluation\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(),\n",
    "        n_estimators = 500,\n",
    "        bootstrap = True,\n",
    "        n_jobs = 1,\n",
    "        oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9904285714285714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/bag_clf_oob.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)\n",
    "print(bag_clf.oob_score_)\n",
    "dump(bag_clf,'models/ch_07/bag_clf_oob.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.988\n"
     ]
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "print(dt_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on test and oob is pretty close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces\n",
    "\n",
    "The ```BaggingClassifier``` supports sampling the features as well. Sampling is controlled by two hyperparamters\n",
    "- max_feautres\n",
    "- bootstrap_features\n",
    "\n",
    "They both work same as max_samples and bootstrap but for feature samplinig instead of instance sampling. This is useful for high dimensional inputs. Sampling both training instances and features is called __random patches method.__ Sampling only features and keeping all instances is called __random subspaces method.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "\n",
    "Random forest is an ensemble of decision trees, generally trained via the bagging method, typically with max_samples set to the size of the trainnig set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "\n",
    "##Paramters\n",
    "#n_jobs: the number of jobs to run in parallel. -1 means  using all processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9863333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/rf_clf.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(rf_clf,'models/ch_07/rf_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest algorithm introduces extra randomness when growing trees, instead of searching for the very best feature when splitting node, it searchesfor the best feature among a random subset of features. \n",
    "\n",
    "The algorithm results in greater tree diversity, which trades a higher bias for a lower vairance, generally yielding a better model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging classifier is roughly eq to random forest\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/bag_clf_rf.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)\n",
    "dump(bag_clf,'models/ch_07/bag_clf_rf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.9846666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "print(dt_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra-Trees\n",
    "\n",
    "It is possible to introduce more randomness by making random thresholds for each feature rather than searhcing for the best possible threshold. \n",
    "\n",
    "A forest of such random trees is called __extremely randomized trees__ ensemple (or __extra trees__) This technique trades more bias for a lower vairance. It's also much faster to train extra trees than random forest, since finding the best threshold for each feature at every node is elimianted. \n",
    "\n",
    "```sklearn``` has a ```ExtraTreesClassifier``` for creating such trees. In general, there is no algo better out of Extra tree or random forest, it's better to check with cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "sklearn computes the feature imp for random forest features automatically, such that all feature imp sum up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rf_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.barh(iris.feature_names, rf_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09259941403225018\n",
      "sepal width (cm) 0.02459758498789982\n",
      "petal length (cm) 0.4396038126017132\n",
      "petal width (cm) 0.4431991883781368\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], rf_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Booting (orginally called __hypothesis boosting__) refers to combining the weak learner into a stronger learner. General idea of boosting is to train the learners sequentially, such that each one corrects the behavior of the previous one. There are many boosting methods available:\n",
    "\n",
    "- AdaBoosting (adaptive boosting)\n",
    "- Gradient Boosting\n",
    "- Extreme gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "\n",
    "Adaboost pays more attention to the training instance that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-ml-book",
   "language": "python",
   "name": "hands-on-ml-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
