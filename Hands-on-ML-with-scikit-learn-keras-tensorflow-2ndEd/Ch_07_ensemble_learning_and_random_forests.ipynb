{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "__Ensemble__\n",
    "\n",
    "A group of predictors is called an ensemble, the technique of grouping the predictors is called ensemble learning, and ensemble learning algorithm is called an ensemble method. \n",
    "\n",
    "Ex. random forest, a group of decision trees is an ensemble method.\n",
    "\n",
    "Different algorithms can be ensembled in different ways, like stacking, boosting and bagging. \n",
    "\n",
    "\n",
    "### Voting Classifiers\n",
    "\n",
    "We can combine different classifiers and select the most predicted class from the result (mode of predictors). Such combining is called voting classifier with hard voting, since the prediction class with most votes from the different classifier is the final classified class. Often, the voting classifier has higher accuracy than individual distinct classifiers. \n",
    "\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy\n",
    "\n",
    "Creating the voting classifier in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "            estimators = [('lr',log_clf),('rf',rf_clf),('svc',svm_clf)],\n",
    "            voting = 'hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples = 10000, noise = 0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.878\n",
      "RandomForestClassifier 0.989\n",
      "SVC 0.9893333333333333\n",
      "VotingClassifier 0.988\n"
     ]
    }
   ],
   "source": [
    "## Checking accuracy for all the classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifiers = [log_clf, rf_clf, svm_clf, voting_clf]\n",
    "\n",
    "for clf in classifiers: \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    dump(clf,''.join(['models/ch_07/',clf.__class__.__name__,'.pkl']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier performs similar to the better classifier. \n",
    "\n",
    "__Soft Voting__\n",
    "\n",
    "If the classifiers have a ```predict_proba()``` class then sklearn can predict class with highest probability, this is called soft voting. It's often better than hard voting because it gives higher weight to high confidence votes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability = True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "            estimators = [('lr',log_clf),('rf',rf_clf),('svc',svm_clf)],\n",
    "            voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.878\n",
      "RandomForestClassifier 0.9873333333333333\n",
      "SVC 0.9893333333333333\n",
      "VotingClassifier 0.9876666666666667\n"
     ]
    }
   ],
   "source": [
    "## Checking accuracy for all the classifiers\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifiers = [log_clf, rf_clf, svm_clf, voting_clf]\n",
    "\n",
    "for clf in classifiers: \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    dump(clf,''.join(['models/ch_07/soft_voting_',clf.__class__.__name__,'.pkl']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "Another method of ensemble is where we train the same algorithm on different subset of the training set. When sampling is performed with replacement, this is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, it's called pasting. both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias\n",
    "and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging and pasting in sklearn\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(),\n",
    "        n_estimators = 500,\n",
    "        max_samples = 100,\n",
    "        bootstrap = True,\n",
    "        n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier 0.978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/bag_clf.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(bag_clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "dump(bag_clf,'models/ch_07/bag_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with Decision Tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/dt_clf.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train[:100],y_train[:100])\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "print(dt_clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "dump(dt_clf,'models/ch_07/dt_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble’s variance is reduced. Overall, bagging often results in better models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Bag Evaluation\n",
    "\n",
    "In bagging, some samples may be sampples several times and some might not be sampled at all. By default, BaggingClassifier samples m trianing instances with replacement where m is the size of the trianing set. Hence some instances are kept oob evaluation, on which trianing is not done. We can evaluate the ensemble by averaging the oob evaluations of each predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing oob evaluation\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "        DecisionTreeClassifier(),\n",
    "        n_estimators = 500,\n",
    "        bootstrap = True,\n",
    "        n_jobs = 1,\n",
    "        oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9904285714285714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/bag_clf_oob.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)\n",
    "print(bag_clf.oob_score_)\n",
    "dump(bag_clf,'models/ch_07/bag_clf_oob.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.988\n"
     ]
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "print(dt_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on test and oob is pretty close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces\n",
    "\n",
    "The ```BaggingClassifier``` supports sampling the features as well. Sampling is controlled by two hyperparamters\n",
    "- max_feautres\n",
    "- bootstrap_features\n",
    "\n",
    "They both work same as max_samples and bootstrap but for feature samplinig instead of instance sampling. This is useful for high dimensional inputs. Sampling both training instances and features is called __random patches method.__ Sampling only features and keeping all instances is called __random subspaces method.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "\n",
    "Random forest is an ensemble of decision trees, generally trained via the bagging method, typically with max_samples set to the size of the trainnig set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "\n",
    "##Paramters\n",
    "#n_jobs: the number of jobs to run in parallel. -1 means  using all processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9863333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/rf_clf.pkl']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(rf_clf,'models/ch_07/rf_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest algorithm introduces extra randomness when growing trees, instead of searching for the very best feature when splitting node, it searchesfor the best feature among a random subset of features. \n",
    "\n",
    "The algorithm results in greater tree diversity, which trades a higher bias for a lower vairance, generally yielding a better model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging classifier is roughly eq to random forest\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/bag_clf_rf.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.fit(X_train,y_train)\n",
    "dump(bag_clf,'models/ch_07/bag_clf_rf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.9846666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "print(dt_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra-Trees\n",
    "\n",
    "It is possible to introduce more randomness by making random thresholds for each feature rather than searhcing for the best possible threshold. \n",
    "\n",
    "A forest of such random trees is called __extremely randomized trees__ ensemple (or __extra trees__) This technique trades more bias for a lower vairance. It's also much faster to train extra trees than random forest, since finding the best threshold for each feature at every node is elimianted. \n",
    "\n",
    "```sklearn``` has a ```ExtraTreesClassifier``` for creating such trees. In general, there is no algo better out of Extra tree or random forest, it's better to check with cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "sklearn computes the feature imp for random forest features automatically, such that all feature imp sum up to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rf_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.barh(iris.feature_names, rf_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09259941403225018\n",
      "sepal width (cm) 0.02459758498789982\n",
      "petal length (cm) 0.4396038126017132\n",
      "petal width (cm) 0.4431991883781368\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], rf_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Booting (orginally called __hypothesis boosting__) refers to combining the weak learner into a stronger learner. General idea of boosting is to train the learners sequentially, such that each one corrects the behavior of the previous one. There are many boosting methods available:\n",
    "\n",
    "- AdaBoosting (adaptive boosting)\n",
    "- Gradient Boosting\n",
    "- Extreme gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost\n",
    "\n",
    "Adaboost pays more attention to the training instance that the predecessor underfitted. This results in new predictors focusing more and more on the hard cases. So if we were trianing a decision tree, ado boost would take a smaple, train it get the weights, then on next turn increase the weight of the misclassified training instances. Then train another classifier and  and update the weight and so on, gradually making it better. \n",
    "\n",
    "Once all the predictors are trianed, the ensemble makes predictions very much like the bagging or pasting, except the predictors have different weights depending on overall accuracy on weighted trianing set. \n",
    "\n",
    "One limitation to the sequantial learning is it cannot be parallelized. Hence, it doesn't scale as well as bagging or pasting. \n",
    "\n",
    "__Maths behind the Adaboost algorithm:__\n",
    "\n",
    "- initially each instance has a weight ($w^i$) 1/m with m as the number of instance. \n",
    "- wgtd error rate ($r_i$) is defined for the predictor as the sum of weight of wrong classification by sum of weights of all instances\n",
    "- the predictor weight is then calculatede with a learning rate hyperparamter (defualt = 1). \n",
    "    - The more accurate the predictor higher the weight is \n",
    "    - on random gues, the weight is closer to 0\n",
    "    - more wrong then right, then the weight is negative\n",
    "    \n",
    "    $ \\alpha_j = \\eta log (\\frac {1-r_j} {r_j}) $ \n",
    "    \n",
    "- Next, adaboost updates the weight for instances\n",
    "    - if the prediction is correct, then weight is same\n",
    "    - if the prediction is wrong, the new weight is old wgt times $exp(\\alpha_j)$\n",
    "- then all instances are normalized (divided the by sum of all weights)\n",
    "- next a new predictor is trained using the new wgts and the proccess is repeated again\n",
    "- The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "            DecisionTreeClassifier(max_depth = 1), \n",
    "                                   n_estimators = 200,\n",
    "                                   algorithm = \"SAMME.R\",\n",
    "                                  learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier 0.9906666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print(ada_clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/ada_clf.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(ada_clf,'models/ch_07/ada_clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Like AdaBoost, Gradient boosting also works by sequentially adding predictors to an ensemble, each one correcting its predecessor. The difference in GB and AB is in the method of deciding the paramters for new predictor. B updates the weight at each instance for new predictor, GB fits the new predictor on the residual errors made by the previous predictor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "##Creating training set\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "##Creating validation set\n",
    "m = 10\n",
    "X_new = 6 * np.random.rand(m, 1) - 3\n",
    "y_new = 0.5 * X_new**2 + X_new + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X,y,\"b.\"  )\n",
    "plt.plot(X_new,y_new,\"y.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/gb_tree_reg_1.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg_1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg_1.fit(X,y)\n",
    "dump(tree_reg_1,'models/ch_07/gb_tree_reg_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/gb_tree_reg_2.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Predicting and training a sequential regressor on the above model\n",
    "\n",
    "y2 = y - tree_reg_1.predict(X)\n",
    "\n",
    "\n",
    "tree_reg_2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg_2.fit(X,y)\n",
    "dump(tree_reg_2,'models/ch_07/gb_tree_reg_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/gb_tree_reg_3.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Predicting and training a sequential regressor on the above model\n",
    "\n",
    "y3 = y2- tree_reg_2.predict(X)\n",
    "\n",
    "\n",
    "tree_reg_3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg_3.fit(X,y)\n",
    "dump(tree_reg_3,'models/ch_07/gb_tree_reg_3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg_1, tree_reg_2, tree_reg_3))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.8471231958223349\n",
      "MSE:  0.3222871101512776\n",
      "RMSE:  0.5677033645763231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "print('R2 Score: ', r2_score(y_pred,y_new))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_new))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1\n",
      "R2 Score:  0.8471231958223349\n",
      "MSE:  0.3222871101512776\n",
      "RMSE:  0.5677033645763231\n",
      "----------------------------\n",
      "Tree 2\n",
      "R2 Score:  0.8471231958223349\n",
      "MSE:  0.3222871101512776\n",
      "RMSE:  0.5677033645763231\n",
      "----------------------------\n",
      "Tree 3\n",
      "R2 Score:  0.8471231958223349\n",
      "MSE:  0.3222871101512776\n",
      "RMSE:  0.5677033645763231\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "##Checking the accuracy of the 3 trees in steps\n",
    "\n",
    "print(\"Tree 1\")\n",
    "y_pred = sum(tree.predict(X_new) for tree in [tree_reg_1])/1\n",
    "\n",
    "print('R2 Score: ', r2_score(y_pred,y_new))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_new))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_new)))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "\n",
    "print(\"Tree 2\")\n",
    "y_pred = sum(tree.predict(X_new) for tree in [tree_reg_1, tree_reg_2])/2\n",
    "\n",
    "print('R2 Score: ', r2_score(y_pred,y_new))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_new))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_new)))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "\n",
    "print(\"Tree 3\")\n",
    "y_pred = sum(tree.predict(X_new) for tree in [tree_reg_1, tree_reg_2,tree_reg_3])/3\n",
    "\n",
    "print('R2 Score: ', r2_score(y_pred,y_new))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_new))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_new)))\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/gbrt.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Usign GBRT from sklearn\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X,y)\n",
    "dump(gbrt,'models/ch_07/gbrt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.8440103351391876\n",
      "MSE:  0.37317804393536474\n",
      "RMSE:  0.6108830034755958\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbrt.predict(X_new)\n",
    "\n",
    "print('R2 Score: ', r2_score(y_pred,y_new))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_new))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_new)))\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate hyperparamter scales the contributions of each tree. If we set it to low value, like 0.1 we need more trees in the ensemble to fit hte trinaing set, but the predictions with generalize better. This is called __shrinkage__ \n",
    "\n",
    "To find optimal number of trees, we can use early stopping. A simple way to implement is to use the ```staged_predict()``` method which returns an iterator oer the predictions made by the ensemble at each stage of training ( with one, two trees) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training 120 trees with GBRT\n",
    "\n",
    "import numpy as no\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "##Creating training set\n",
    "m = 10000\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/gbrt_120.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_120 = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt_120.fit(X_train,y_train)\n",
    "dump(gbrt_120, 'models/ch_07/gbrt_120.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.7290609838498993\n",
      "MSE:  1.275234656957206\n",
      "RMSE:  1.1292628821302886\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbrt.predict(X_test)\n",
    "\n",
    "print('R2 Score: ', r2_score(y_pred,y_test))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_test))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_test)))\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\apurv\\anaconda3\\envs\\hands-on-ml-book\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:494: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "##Implementing early stopping using warm_start = True\n",
    "\n",
    "gbrt_es = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1,120):\n",
    "    gbrt_es.n_estimators = n_estimators\n",
    "    gbrt_es.fit(X_train,y_train)\n",
    "    y_pred = gbrt_es.predict(X_test)\n",
    "    val_error = mean_squared_error(y_pred,y_test)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break ##Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9710140246424315"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_error\n",
    "##Better then the full 120 model runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/gbrt_es.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(gbrt_es,'models/ch_07/gbrt_es.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stochastic Gradient Boosting__\n",
    "\n",
    "```GradientBoostingRegressor``` class also support a subsample hyperparamter, which specifies the fraction of training instances to be used for training each tree. This technique trades a high bias for a lower variance and speeds up the training considerably. \n",
    "\n",
    "It is possible to use gradient boosting with other cost functions. This is controlled by loss hyperparamter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/xgb_reg.pkl']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(xgb_reg,'models/ch_07/xgb_reg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.7939054500451354\n",
      "MSE:  1.0424502417151227\n",
      "RMSE:  1.0210045258054063\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "print('R2 Score: ', r2_score(y_pred,y_test))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_test))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_test)))\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:2.77058\n",
      "[1]\tvalidation_0-rmse:2.05706\n",
      "[2]\tvalidation_0-rmse:1.59780\n",
      "[3]\tvalidation_0-rmse:1.31614\n",
      "[4]\tvalidation_0-rmse:1.15583\n",
      "[5]\tvalidation_0-rmse:1.07032\n",
      "[6]\tvalidation_0-rmse:1.02825\n",
      "[7]\tvalidation_0-rmse:1.00709\n",
      "[8]\tvalidation_0-rmse:0.99908\n",
      "[9]\tvalidation_0-rmse:0.99435\n",
      "[10]\tvalidation_0-rmse:0.99329\n",
      "[11]\tvalidation_0-rmse:0.99284\n",
      "[12]\tvalidation_0-rmse:0.99316\n",
      "[13]\tvalidation_0-rmse:0.99278\n",
      "[14]\tvalidation_0-rmse:0.99336\n",
      "[15]\tvalidation_0-rmse:0.99371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/xgb_reg_es.pkl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##XGBoost has a inbuilt early stopping hyperparamter\n",
    "\n",
    "xgb_reg.fit(X_train, y_train, eval_set = [(X_test,y_test)], early_stopping_rounds = 2)\n",
    "dump(xgb_reg,'models/ch_07/xgb_reg_es.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.7985261282796258\n",
      "MSE:  0.9856108075624117\n",
      "RMSE:  0.9927793347780823\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_reg.predict(X_test)\n",
    "\n",
    "print('R2 Score: ', r2_score(y_pred,y_test))\n",
    "print('MSE: ', mean_squared_error(y_pred,y_test))\n",
    "print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_test)))\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping in XGB gives better result than full model run\n",
    "\n",
    "\n",
    "### Stacking\n",
    "\n",
    "stacking is short for stacked generalization. It's based on a simple idea, instead of using trivial functions like hard voting to aggregate predictions of all predictors in an ensemble we should train a model to perform this aggregation. There are several distinct predictors and then a belnder, which aggregates the predictors. The blender is trained on a __hold-out set__ after dividing the training set into two. This ensures the predictors are clean, since they never saw this data during training. \n",
    "\n",
    "The new training set can also be created by combining the predicted values from all the predictors. It's actually possible to train several different blenders this way, to get a whole layer of blenders. The trick is to split the dataset into 3 parts. ffirst is used to train the predictors, second is used to create the new training set, third is used to create training set for the third layer. Once this is done, we can make the predictions for a new instance by going through each layer sequentially. \n",
    "\n",
    "```sklearn doesn't support stacking directly, we can do manual implementation (in exercises) or use an open source implementation like brew``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "\n",
    "__1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?__\n",
    "\n",
    "If all the models are high performing we can combine them using hard voting, with take the mode of the prediction results to get the final prediction. Hard voting generally outperforms the best performing model and can slightly increase the performance metrics.  \n",
    "\n",
    "\n",
    "__2. What is the difference between hard and soft voting classifiers?__\n",
    "\n",
    "hard voting works well when the models are different and genreate the class prediction as the final predictions, har voting then selects the mode of the predictions and classifies that as the final prediction. Soft voting works well when the classification algos can generate the probabilty scores for each class, hence instead of taking the mode of the prediction classes it takes into account the higher confidence and performs slighly better than hard voting classifiers.\n",
    "\n",
    "\n",
    "__3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, Random Forests, or stacking ensembles?__\n",
    "\n",
    "Yes, bagging uses the result of individually trained trees and then combines the result of these to get hte final prediction, hence they can be parallelized or distributed into different servers for faster computation.\n",
    "\n",
    "Random forest is based on bagging which training individual decision trees parallely and averages the result for final prediction, same with pasting. These both can run parallely and hence are faster to train. Boosting and Stacking ensembles train sequentially, hence they are dependent on the output of the previous predictor which cannot be parallized resulting in more trianing time.  \n",
    "\n",
    "\n",
    "__4. What is the benefit of out-of-bag evaluation?__\n",
    "\n",
    "out-of-bad evaluation is done on the new data that the predictor is not trained on, hence during training itself, the predictor can judge the perofrmance and correct itself without waitning for the full model training and then evaluation on the validation dataset.\n",
    "\n",
    "\n",
    "__5. What makes Extra-Trees more random than regular Random Forests? How can\n",
    "this extra randomness help? Are Extra-Trees slower or faster than regular\n",
    "Random Forests?__\n",
    "\n",
    "Extra trees select the features on random for each subset and hence doesn't wait for the selection of the best features for the particular dataset, and a optimal threshold for each tree resulting in faster trianing. This adds an extra randomness to the forest trees and acts like a form of regularization.  \n",
    "\n",
    "\n",
    "__6. If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?__\n",
    "\n",
    "We can increase the estimators for the adaboost in case of underfitting or reduce the regularization which may be constraining the predictor. We can increase the learning rate as well, which can increase the predictor weight for more accurate predictor, amplifying the correct predictions.  \n",
    "\n",
    "\n",
    "__7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?__\n",
    "\n",
    "we should decrease the learning rate if the GB is overfitting the dataset, we can aslo try early stopping.\n",
    "\n",
    "\n",
    "__8. Step wise__\n",
    "\n",
    "__a. Load the MNIST data (introduced in Chapter 3), and__\n",
    "\n",
    "__b. split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing).__\n",
    "\n",
    "__c. Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM classifier.__\n",
    "\n",
    "__d. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting.__\n",
    "\n",
    "__e. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Loading the dataset\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version = 1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = mnist[\"data\"],mnist[\"target\"]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = X[:50000], X[50000:60000], X[60000:],y[:50000], y[50000:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training ExtraTreesClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et_clf = ExtraTreesClassifier()\n",
    "et_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Trianing SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/ex_svc_clf.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Saving the individual models\n",
    "from joblib import dump\n",
    "\n",
    "dump(rf_clf,'models/ch_07/ex_rf_clf.pkl')\n",
    "dump(et_clf,'models/ch_07/ex_et_clf.pkl')\n",
    "dump(svc_clf,'models/ch_07/ex_svc_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.9724\n",
      "ExtraTreesClassifier 0.9743\n",
      "SVC 0.9802\n"
     ]
    }
   ],
   "source": [
    "##Evaluaing individual models on val set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clfs = [rf_clf,et_clf,svc_clf]\n",
    "\n",
    "for clf in clfs:\n",
    "    y_pred = clf.predict(X_val)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_pred,y_val))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier()),\n",
       "                             ('et', ExtraTreesClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf_hv = VotingClassifier(\n",
    "            estimators = [('rf',rf_clf),('et',et_clf),('svc',svc_clf)],\n",
    "            voting = 'hard')\n",
    "\n",
    "voting_clf_hv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/ex_voting_clf_hv.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = voting_clf_hv.predict(X_val)\n",
    "print(voting_clf_hv.__class__.__name__, accuracy_score(y_pred,y_val))\n",
    "dump(voting_clf_hv,'models/ch_07/ex_voting_clf_hv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Soft Voting__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier()),\n",
       "                             ('et', ExtraTreesClassifier()),\n",
       "                             ('svc', SVC(probability=True))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf_sv = VotingClassifier(\n",
    "            estimators = [('rf',rf_clf),('et',et_clf),('svc',SVC(probability = True))],\n",
    "            voting = 'soft')\n",
    "\n",
    "voting_clf_sv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.9809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models/ch_07/ex_voting_clf_sv.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = voting_clf_sv.predict(X_val)\n",
    "print(voting_clf_sv.__class__.__name__, accuracy_score(y_pred,y_val))\n",
    "dump(voting_clf_sv,'models/ch_07/ex_voting_clf_sv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.9691\n",
      "ExtraTreesClassifier 0.9714\n",
      "SVC 0.9785\n",
      "VotingClassifier 0.9735\n",
      "VotingClassifier 0.9784\n"
     ]
    }
   ],
   "source": [
    "## Evaluation on test data\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clfs = [rf_clf,et_clf,svc_clf,voting_clf_hv,voting_clf_sv]\n",
    "\n",
    "for clf in clfs:\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_pred,y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__9. Step wise__\n",
    "\n",
    "__a. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class.__\n",
    "\n",
    "__b. Train a classifier on this new training set.__\n",
    "\n",
    "__Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble!__\n",
    "\n",
    "__c. Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions.__\n",
    "\n",
    "__d. How does it compare to the voting classifier you trained earlier?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "rf_clf = load('models/ch_07/ex_rf_clf.pkl')\n",
    "et_clf = load('models/ch_07/ex_et_clf.pkl')\n",
    "svc_clf = load('models/ch_07/ex_svc_clf.pkl')\n",
    "voting_clf_sv = load('models/ch_07/ex_voting_clf_sv.pkl')\n",
    "voting_clf_hv = load('models/ch_07/ex_voting_clf_hv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new = []\n",
    "\n",
    "for i in range(0,len(X_val)):\n",
    "    t = X_val[i:i+1]\n",
    "    y_pred_rf = rf_clf.predict(t)\n",
    "    y_pred_et = et_clf.predict(t)\n",
    "    y_pred_svc = svc_clf.predict(t)\n",
    "    X_train_new.append([y_pred_rf[0],y_pred_et[0],y_pred_svc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '3', '3']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##SVC as blender\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_blender  = SVC()\n",
    "svc_blender.fit(X_train_new, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##RF as blender\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_blender = RandomForestClassifier(n_estimators=10)\n",
    "rf_blender.fit(X_train_new, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/ex_svc_blender.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(svc_blender,'models/ch_07/ex_svc_blender.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ch_07/ex_rf_blender.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(rf_blender,'models/ch_07/ex_rf_blender.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = []\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    t = X_test[i:i+1]\n",
    "    y_pred_rf = rf_clf.predict(t)\n",
    "    y_pred_et = et_clf.predict(t)\n",
    "    y_pred_svc = svc_clf.predict(t)\n",
    "    X_test_new.append([y_pred_rf[0],y_pred_et[0],y_pred_svc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9697"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = svc_blender.predict(X_test_new)\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9751"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_blender.predict(X_test_new)\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Accuracy on the Final Test set are:\n",
    "\n",
    "- RandomForestClassifier 0.9691\n",
    "- ExtraTreesClassifier 0.9714\n",
    "- SVC 0.9785\n",
    "- Hard VotingClassifier 0.9735\n",
    "- Soft VotingClassifier 0.9784\n",
    "- SCV blender Stacked Classifier 0.9697\n",
    "- RandomForest blender Stacked Classifier 0.9751\n",
    "\n",
    "\n",
    "In individual classifiers, SVC performed best with accuracy of 97.85%\n",
    "Using Voting, Hard voting peroforms better then Random forest and Extra tree classifier, Soft voting performs similar to SVC.\n",
    "Using Stacking with SVC blender the accuracy is slightly lower at 96.97%, stacking with random forest classifier is higher than hard voting but lower than soft voting at 97.51% "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-ml-book",
   "language": "python",
   "name": "hands-on-ml-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
