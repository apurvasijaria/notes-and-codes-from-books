{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Networks are much more complex than the shallow NN trained in last chapter. Issues in trianing DNN:\n",
    "\n",
    "- vanishing graidents or exploding gradients\n",
    "- not enough training data\n",
    "- slow training\n",
    "- overfitting the trianing set\n",
    "\n",
    "\n",
    "### Vanishing and Exploding Gradients\n",
    "\n",
    "In backpropogation, the algo works by going from the output layer to the input layer and propogating the error gradient along the way. gradients often get small and smaller as the algorithm progressed down to the lower layers. resulting in no change in the lower layer leaved and training never converging to a good solution. this is called vanishing gradietns. \n",
    "\n",
    "In some cases, opposite happends, the gradients grow bigger and bigger until the layers get insanely large weight updates andthe algo diverges, this is called exploding gradients, which resurfaces in recurrent NN. \n",
    "\n",
    "MMore generaly, DNN suffer from unstable gradients, different layers may learn at widely different speeds.\n",
    "\n",
    "This was concluded in a 2010 study, that sigmoid function saturates at 0 or 1, hence the sgd has no gradient to propogate back from. \n",
    "\n",
    "__Glorot and He Initialization__\n",
    "\n",
    "for signifficantly allecviate the unstable gradient problem. we need to signal the flow properly in both directions: in the forward direction when making the predictions and in reverse direction when backpropogating gradients. we don't want the signal to die out or explode and saturate. for the signal to flow properly, we need the vairance of the outputs of each layer to be equal to the variance of its inputs and we need the graideitns tp have equal variance before and after flowing through a layer in reverse direction. it's not possible unless the layer has an equal number of input and neurons (fan in and fan out of the layer), but glorot and bengio proposed a compromize that is proven to work in ractive. the connection weights of each layer must be initialized randomly as ${fan}_{avg} = ({fan}_{in} + {fan}_{out})/2$.  this is called _xavier initialization or glorot initializtion_ \n",
    "\n",
    "\n",
    "| Initialization | Activation functions | σ² (Normal)|\n",
    "| :-: | :-: | :-: |\n",
    "| Glorot |  None, tanh, logistic, softmax | $ \\frac 1 {{fan}_{avg}}$|\n",
    "| He | ReLU and variants | $ \\frac 2 {{fan}_{in}}$ |\n",
    "| LeCun | SELU | $ \\frac 1 {{fan}_{in}}$|\n",
    "\n",
    "\n",
    "By default, Keras uses Glorot initialization with a uniform distribution. When creating a layer, you can change this to He initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fmnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating validation dataset and scaling the dataset\n",
    "\n",
    "X_val, X_train = X_train[:5000],X_train[5000:]\n",
    "y_val, y_train = y_train[:5000],y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = X_train/255.0, X_val/255.0,X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##labels from https://keras.io/api/datasets/fashion_mnist/#loaddata-function\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.Dense(100, activation=\"relu\",kernel_initializer = \"he_normal\"),\n",
    "keras.layers.Dense(100, activation=\"relu\",kernel_initializer = \"he_normal\"),\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 5s 2ms/step - loss: 0.7203 - accuracy: 0.7575 - val_loss: 0.5084 - val_accuracy: 0.8266\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4945 - accuracy: 0.8271 - val_loss: 0.4685 - val_accuracy: 0.8432\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4498 - accuracy: 0.8420 - val_loss: 0.4171 - val_accuracy: 0.8606\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4232 - accuracy: 0.8505 - val_loss: 0.4685 - val_accuracy: 0.8312\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.4024 - accuracy: 0.8602 - val_loss: 0.3893 - val_accuracy: 0.8634\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3889 - accuracy: 0.8628 - val_loss: 0.3890 - val_accuracy: 0.8642\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3749 - accuracy: 0.8686 - val_loss: 0.3595 - val_accuracy: 0.8746\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3633 - accuracy: 0.8714 - val_loss: 0.3646 - val_accuracy: 0.8700\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3530 - accuracy: 0.8756 - val_loss: 0.3781 - val_accuracy: 0.8696\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3444 - accuracy: 0.8770 - val_loss: 0.3663 - val_accuracy: 0.8714\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3369 - accuracy: 0.8796 - val_loss: 0.3644 - val_accuracy: 0.8726\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3300 - accuracy: 0.8811 - val_loss: 0.3346 - val_accuracy: 0.8806\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3221 - accuracy: 0.8833 - val_loss: 0.3298 - val_accuracy: 0.8792\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3159 - accuracy: 0.8861 - val_loss: 0.3386 - val_accuracy: 0.8778\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3108 - accuracy: 0.8882 - val_loss: 0.3454 - val_accuracy: 0.8740\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3048 - accuracy: 0.8904 - val_loss: 0.3301 - val_accuracy: 0.8774\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3001 - accuracy: 0.8918 - val_loss: 0.3297 - val_accuracy: 0.8830\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2954 - accuracy: 0.8935 - val_loss: 0.3212 - val_accuracy: 0.8836\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2910 - accuracy: 0.8944 - val_loss: 0.3641 - val_accuracy: 0.8708\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2865 - accuracy: 0.8952 - val_loss: 0.3202 - val_accuracy: 0.8872\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2813 - accuracy: 0.8982 - val_loss: 0.3303 - val_accuracy: 0.8794\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2778 - accuracy: 0.8999 - val_loss: 0.3174 - val_accuracy: 0.8854\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2736 - accuracy: 0.9015 - val_loss: 0.3232 - val_accuracy: 0.8824\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2692 - accuracy: 0.9026 - val_loss: 0.3059 - val_accuracy: 0.8884\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2661 - accuracy: 0.9034 - val_loss: 0.3128 - val_accuracy: 0.8866\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2629 - accuracy: 0.9045 - val_loss: 0.3180 - val_accuracy: 0.8858\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2590 - accuracy: 0.9052 - val_loss: 0.3167 - val_accuracy: 0.8884\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2559 - accuracy: 0.9070 - val_loss: 0.3217 - val_accuracy: 0.8794\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2523 - accuracy: 0.9081 - val_loss: 0.3143 - val_accuracy: 0.8880\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2498 - accuracy: 0.9098 - val_loss: 0.3070 - val_accuracy: 0.8908\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/ch_11/keras_gerlot_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3410279452800751, 0.8769000172615051]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want He initialization with a uniform distribution but based on ${fan}_{avg}$ rather than ${fan}_{in}$ , you can use the VarianceScaling initializer like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution = \"uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.Dense(100, activation=\"sigmoid\",kernel_initializer = he_avg_init),\n",
    "keras.layers.Dense(100, activation=\"sigmoid\",kernel_initializer = he_avg_init),\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 1.9201 - accuracy: 0.4789 - val_loss: 1.4934 - val_accuracy: 0.5956\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 1.2520 - accuracy: 0.6496 - val_loss: 1.0548 - val_accuracy: 0.7136\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.9580 - accuracy: 0.7067 - val_loss: 0.8606 - val_accuracy: 0.7288\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8094 - accuracy: 0.7319 - val_loss: 0.7483 - val_accuracy: 0.7530\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.7225 - accuracy: 0.7475 - val_loss: 0.6813 - val_accuracy: 0.7596\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6677 - accuracy: 0.7601 - val_loss: 0.6363 - val_accuracy: 0.7738\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6295 - accuracy: 0.7722 - val_loss: 0.6022 - val_accuracy: 0.7854\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5999 - accuracy: 0.7835 - val_loss: 0.5744 - val_accuracy: 0.7980\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5756 - accuracy: 0.7939 - val_loss: 0.5522 - val_accuracy: 0.8052\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5549 - accuracy: 0.8031 - val_loss: 0.5312 - val_accuracy: 0.8164\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5373 - accuracy: 0.8097 - val_loss: 0.5147 - val_accuracy: 0.8220\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5221 - accuracy: 0.8154 - val_loss: 0.5038 - val_accuracy: 0.8236\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5092 - accuracy: 0.8199 - val_loss: 0.4901 - val_accuracy: 0.8344\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4978 - accuracy: 0.8247 - val_loss: 0.4815 - val_accuracy: 0.8358\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4878 - accuracy: 0.8283 - val_loss: 0.4723 - val_accuracy: 0.8384\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4793 - accuracy: 0.8309 - val_loss: 0.4632 - val_accuracy: 0.8386\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4714 - accuracy: 0.8328 - val_loss: 0.4558 - val_accuracy: 0.8400\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4645 - accuracy: 0.8355 - val_loss: 0.4491 - val_accuracy: 0.8428\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4581 - accuracy: 0.8373 - val_loss: 0.4446 - val_accuracy: 0.8450\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4524 - accuracy: 0.8398 - val_loss: 0.4421 - val_accuracy: 0.8472\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4471 - accuracy: 0.8422 - val_loss: 0.4356 - val_accuracy: 0.8498\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4422 - accuracy: 0.8435 - val_loss: 0.4323 - val_accuracy: 0.8528\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4375 - accuracy: 0.8450 - val_loss: 0.4276 - val_accuracy: 0.8512\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4331 - accuracy: 0.8467 - val_loss: 0.4241 - val_accuracy: 0.8530\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4290 - accuracy: 0.8479 - val_loss: 0.4201 - val_accuracy: 0.8546\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4252 - accuracy: 0.8488 - val_loss: 0.4179 - val_accuracy: 0.8554\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4214 - accuracy: 0.8503 - val_loss: 0.4173 - val_accuracy: 0.8570\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4180 - accuracy: 0.8517 - val_loss: 0.4147 - val_accuracy: 0.8562\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4146 - accuracy: 0.8528 - val_loss: 0.4099 - val_accuracy: 0.8586\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4112 - accuracy: 0.8544 - val_loss: 0.4067 - val_accuracy: 0.8600\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/ch_11/keras_gerlot_favg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.8375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44819024205207825, 0.8374999761581421]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "\n",
    "In 2010 paper by Glorot and Bengio, insights was that the problem with the unstable gradient was the poor choice of activation function. it turned out that activation function other than sigmoid, like relu work much better in deepl learning becaudse it does not saturate and is faster to compute. \n",
    "\n",
    "relu suffers from a problem called dying relu, that is during training some neurons effectively 'die' meaning they stop outputing anythin other than 0. in some cases we find that half of the neurons are dead, especially with high leanring rate. to solve this we use leaky relu. it's defined as max(az,z) parameter a defines how much the relu leaks, it is the slop of z<0, smaller the slop lesser the leak. it can go into long sleep but wakes up after a few layers. \n",
    "\n",
    "in comparision, leaky relu outperform relus. and leaky relu with huge leak perform better than smaller leak relu. randomized leaky relu, where the leak is decided on random also perform fair. paramteric leaky relu where a is authorized to be learned during training. \n",
    "\n",
    "__Exponential Linear Unit (ELU)__\n",
    "\n",
    "\n",
    "exponential linear unit (ELU) outperforms relu variants, with reduced training time.  \n",
    "\n",
    "${ELU}_α (z) = { α(exp (z) − 1) if z < 0  z if z ≥ 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELU activation function looks a lot like the ReLU function, with a few\n",
    "major differences:\n",
    "\n",
    "- It takes on negative values when z < 0, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. - It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
    "- If α is equal to 1 then the function is smooth everywhere, including around z = 0, which helps speed up Gradient Descent since it does not bounce as much to the left and right of z = 0.\n",
    "\n",
    "The main drawback of the ELU activation function is that it is slower to\n",
    "compute than the ReLU function and its variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Scaled ELU activation function__\n",
    "\n",
    "if all hidden layers use the SELU activation function, then the network will selfnormalize: the output of each layer will tend to preserve a mean of 0 and\n",
    "standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.Dense(100,kernel_initializer = \"he_normal\"),\n",
    "keras.layers.LeakyReLU(alpha = 0.2),\n",
    "keras.layers.Dense(100,activation=\"selu\",kernel_initializer=\"lecun_normal\"),\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.6548 - accuracy: 0.7805 - val_loss: 0.4962 - val_accuracy: 0.8258\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4712 - accuracy: 0.8338 - val_loss: 0.4319 - val_accuracy: 0.8526\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4350 - accuracy: 0.8463 - val_loss: 0.4221 - val_accuracy: 0.8532\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4146 - accuracy: 0.8518 - val_loss: 0.3943 - val_accuracy: 0.8622\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3985 - accuracy: 0.8583 - val_loss: 0.3956 - val_accuracy: 0.8608\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3846 - accuracy: 0.8638 - val_loss: 0.3702 - val_accuracy: 0.8746\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3744 - accuracy: 0.8676 - val_loss: 0.3632 - val_accuracy: 0.8744\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3634 - accuracy: 0.8717 - val_loss: 0.3812 - val_accuracy: 0.8624\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3558 - accuracy: 0.8725 - val_loss: 0.3564 - val_accuracy: 0.8752\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3468 - accuracy: 0.8758 - val_loss: 0.3613 - val_accuracy: 0.8734\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3405 - accuracy: 0.8775 - val_loss: 0.3565 - val_accuracy: 0.8736\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3346 - accuracy: 0.8795 - val_loss: 0.3458 - val_accuracy: 0.8778\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3279 - accuracy: 0.8822 - val_loss: 0.3380 - val_accuracy: 0.8794\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3220 - accuracy: 0.8848 - val_loss: 0.3358 - val_accuracy: 0.8794\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3173 - accuracy: 0.8863 - val_loss: 0.3511 - val_accuracy: 0.8738\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3121 - accuracy: 0.8876 - val_loss: 0.3377 - val_accuracy: 0.8818\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3066 - accuracy: 0.8891 - val_loss: 0.3356 - val_accuracy: 0.8774\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3017 - accuracy: 0.8914 - val_loss: 0.3496 - val_accuracy: 0.8778\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2972 - accuracy: 0.8923 - val_loss: 0.3254 - val_accuracy: 0.8812\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2932 - accuracy: 0.8949 - val_loss: 0.3317 - val_accuracy: 0.8830\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2889 - accuracy: 0.8952 - val_loss: 0.3264 - val_accuracy: 0.8836\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2851 - accuracy: 0.8975 - val_loss: 0.3323 - val_accuracy: 0.8816\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2808 - accuracy: 0.8984 - val_loss: 0.3259 - val_accuracy: 0.8848\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2782 - accuracy: 0.9003 - val_loss: 0.3160 - val_accuracy: 0.8856\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2743 - accuracy: 0.9008 - val_loss: 0.3254 - val_accuracy: 0.8838\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2722 - accuracy: 0.9009 - val_loss: 0.3158 - val_accuracy: 0.8850\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2688 - accuracy: 0.9031 - val_loss: 0.3136 - val_accuracy: 0.8866\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2642 - accuracy: 0.9035 - val_loss: 0.3333 - val_accuracy: 0.8802\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2618 - accuracy: 0.9053 - val_loss: 0.3085 - val_accuracy: 0.8888\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2591 - accuracy: 0.9062 - val_loss: 0.3131 - val_accuracy: 0.8908\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/ch_11/keras_relu_selu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3496 - accuracy: 0.8740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34964653849601746, 0.8740000128746033]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "BN is done before and after a each hidden layer, where the input is zero centered and normalized and output is shifted using the two new paramter vectors per layer, one for scaling and another for shifting. In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”). Batch Normalization acts like a regularizer, reducing the need for other regularization techniques.\n",
    "\n",
    "\n",
    "Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it’s often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset. the training time is slow but the convergence is faster, hence the epoches required for opitmal solution is lesser. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation = \"elu\", kernel_initializer = \"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##paramters for first bn layer\n",
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dense/kernel:0', True), ('dense/bias:0', True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[2].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_1/gamma:0', True),\n",
       " ('batch_normalization_1/beta:0', True),\n",
       " ('batch_normalization_1/moving_mean:0', False),\n",
       " ('batch_normalization_1/moving_variance:0', False)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[3].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 15s 6ms/step - loss: 0.5524 - accuracy: 0.8062 - val_loss: 0.4094 - val_accuracy: 0.8600\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4248 - accuracy: 0.8497 - val_loss: 0.3795 - val_accuracy: 0.8684\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3871 - accuracy: 0.8639 - val_loss: 0.3587 - val_accuracy: 0.8718\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3649 - accuracy: 0.8699 - val_loss: 0.3458 - val_accuracy: 0.8756\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3455 - accuracy: 0.8765 - val_loss: 0.3418 - val_accuracy: 0.8784\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3291 - accuracy: 0.8822 - val_loss: 0.3278 - val_accuracy: 0.8814\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3186 - accuracy: 0.8837 - val_loss: 0.3278 - val_accuracy: 0.8820\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3057 - accuracy: 0.8904 - val_loss: 0.3250 - val_accuracy: 0.8842\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2971 - accuracy: 0.8923 - val_loss: 0.3214 - val_accuracy: 0.8822\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2898 - accuracy: 0.8938 - val_loss: 0.3169 - val_accuracy: 0.8860\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2797 - accuracy: 0.8976 - val_loss: 0.3234 - val_accuracy: 0.8806\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2715 - accuracy: 0.9018 - val_loss: 0.3186 - val_accuracy: 0.8854\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2655 - accuracy: 0.9029 - val_loss: 0.3134 - val_accuracy: 0.8836\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2570 - accuracy: 0.9056 - val_loss: 0.3118 - val_accuracy: 0.8904\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2505 - accuracy: 0.9094 - val_loss: 0.3151 - val_accuracy: 0.8878\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2462 - accuracy: 0.9111 - val_loss: 0.3074 - val_accuracy: 0.8886\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2384 - accuracy: 0.9128 - val_loss: 0.3073 - val_accuracy: 0.8912\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2311 - accuracy: 0.9166 - val_loss: 0.3158 - val_accuracy: 0.8850\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2279 - accuracy: 0.9179 - val_loss: 0.3076 - val_accuracy: 0.8884\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2214 - accuracy: 0.9198 - val_loss: 0.3060 - val_accuracy: 0.8906\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2185 - accuracy: 0.9194 - val_loss: 0.2999 - val_accuracy: 0.8926\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2139 - accuracy: 0.9224 - val_loss: 0.3154 - val_accuracy: 0.8878\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2085 - accuracy: 0.9230 - val_loss: 0.3092 - val_accuracy: 0.8880\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2037 - accuracy: 0.9259 - val_loss: 0.3109 - val_accuracy: 0.8920\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1997 - accuracy: 0.9269 - val_loss: 0.3208 - val_accuracy: 0.8870\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1930 - accuracy: 0.9299 - val_loss: 0.3058 - val_accuracy: 0.8938\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1915 - accuracy: 0.9303 - val_loss: 0.3120 - val_accuracy: 0.8892\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.1880 - accuracy: 0.9315 - val_loss: 0.3166 - val_accuracy: 0.8942\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1842 - accuracy: 0.9336 - val_loss: 0.3100 - val_accuracy: 0.8936\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1809 - accuracy: 0.9334 - val_loss: 0.3267 - val_accuracy: 0.8912\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3493 - accuracy: 0.8855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3493002951145172, 0.8855000138282776]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_val))\n",
    "model.save('models/ch_11/keras_bn_1.h5')\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add the BN layers before the activation functions, we must remove the activation function from the hidden layers and add them as separate layers after the BN layers. Batch Normalization layer includes one offset parameter per input, we can remove the bias term from the previous layer (just pass use_bias=False when creating it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"elu\"),\n",
    "keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"elu\"),\n",
    "keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5660 - accuracy: 0.8062 - val_loss: 0.4278 - val_accuracy: 0.8516\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4406 - accuracy: 0.8447 - val_loss: 0.3936 - val_accuracy: 0.8676\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4071 - accuracy: 0.8558 - val_loss: 0.3789 - val_accuracy: 0.8704\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.3838 - accuracy: 0.8643 - val_loss: 0.3620 - val_accuracy: 0.8752\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3652 - accuracy: 0.8691 - val_loss: 0.3536 - val_accuracy: 0.8754\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3498 - accuracy: 0.8763 - val_loss: 0.3470 - val_accuracy: 0.8762\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3372 - accuracy: 0.8800 - val_loss: 0.3419 - val_accuracy: 0.8802\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3268 - accuracy: 0.8823 - val_loss: 0.3462 - val_accuracy: 0.8790\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3152 - accuracy: 0.8875 - val_loss: 0.3371 - val_accuracy: 0.8820\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3064 - accuracy: 0.8899 - val_loss: 0.3311 - val_accuracy: 0.8848\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2980 - accuracy: 0.8917 - val_loss: 0.3327 - val_accuracy: 0.8822\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2901 - accuracy: 0.8967 - val_loss: 0.3278 - val_accuracy: 0.8830\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2814 - accuracy: 0.8983 - val_loss: 0.3294 - val_accuracy: 0.8846\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2748 - accuracy: 0.9012 - val_loss: 0.3207 - val_accuracy: 0.8846\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2685 - accuracy: 0.9017 - val_loss: 0.3167 - val_accuracy: 0.8862\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2598 - accuracy: 0.9056 - val_loss: 0.3325 - val_accuracy: 0.8830\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2550 - accuracy: 0.9078 - val_loss: 0.3164 - val_accuracy: 0.8868\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2510 - accuracy: 0.9091 - val_loss: 0.3267 - val_accuracy: 0.8864\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2438 - accuracy: 0.9111 - val_loss: 0.3299 - val_accuracy: 0.8844\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2398 - accuracy: 0.9138 - val_loss: 0.3161 - val_accuracy: 0.8864\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2331 - accuracy: 0.9158 - val_loss: 0.3137 - val_accuracy: 0.8874\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2313 - accuracy: 0.9167 - val_loss: 0.3128 - val_accuracy: 0.8920\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2247 - accuracy: 0.9179 - val_loss: 0.3205 - val_accuracy: 0.8884\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2192 - accuracy: 0.9206 - val_loss: 0.3136 - val_accuracy: 0.8872\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2179 - accuracy: 0.9207 - val_loss: 0.3180 - val_accuracy: 0.8870\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2107 - accuracy: 0.9242 - val_loss: 0.3124 - val_accuracy: 0.8904\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2063 - accuracy: 0.9248 - val_loss: 0.3132 - val_accuracy: 0.8922\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2040 - accuracy: 0.9269 - val_loss: 0.3121 - val_accuracy: 0.8898\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1987 - accuracy: 0.9275 - val_loss: 0.3143 - val_accuracy: 0.8882\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1949 - accuracy: 0.9301 - val_loss: 0.3295 - val_accuracy: 0.8856\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3542 - accuracy: 0.8816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35423582792282104, 0.881600022315979]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data = (X_val, y_val))\n",
    "model.save('models/ch_11/keras_bn_2.h5')\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__BN hyperparameters__\n",
    "\n",
    "1. Momentum\n",
    "The BatchNormalization class has quite a few hyperparameters we can tweak. The defaults will usually be fine, but you may occasionally need to tweak the momentum. This hyperparameter is used by the BatchNormalization layer when it updates the exponential moving averages; given a new value v. A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999\n",
    "\n",
    "2. Axis\n",
    "it determines which axis should be normalized. It defaults to –1, meaning that by default it will normalize the last axis. \n",
    "\n",
    "### Gradient Clipping\n",
    "\n",
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. It is often used in RNN, since BN is tricky to use. In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer.\n",
    "\n",
    "`optimizer = keras.optimizers.SGD(clipvalue=1.0)`\n",
    "`model.compile(loss=\"mse\", optimizer=optimizer)`\n",
    "\n",
    "to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue.\n",
    "\n",
    "\n",
    "### Reusing Pretrained Layers\n",
    "\n",
    "we don't usually train the neural network from scratch, we often use the lower layers from a already trained model and build upon it. it's called transfer learning. it reduces the training time and requires very less trianing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer of the original model should usually be replaced because it\n",
    "is most likely not useful at all for the new task, and it may not even have the\n",
    "right number of outputs for the new task. Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. we want to find the right number of layers to reuse. The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing\n",
    "the output layer.\n",
    "\n",
    "#### Transfer Learning with Keras\n",
    "\n",
    "load model A and create a new model based on that model’s layers. Let’s reuse all the layers except for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"models/ch_11/keras_bn_2.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone model_A before you reuse its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights. To avoid this, one approach is to freeze the reused layers during the first few epochs, giving the new layer some time to learn reasonable weights. To do this, set every layer’s trainable attribute to False and compile the model. we must always compile your model after you freeze or unfreeze layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1719/1719 [==============================] - 8s 3ms/step - loss: 0.5014 - accuracy: 0.8434 - val_loss: 0.3561 - val_accuracy: 0.8744\n",
      "Epoch 2/4\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2393 - accuracy: 0.9237 - val_loss: 0.3227 - val_accuracy: 0.8828\n",
      "Epoch 3/4\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2038 - accuracy: 0.9345 - val_loss: 0.3122 - val_accuracy: 0.8880\n",
      "Epoch 4/4\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1860 - accuracy: 0.9403 - val_loss: 0.3090 - val_accuracy: 0.8868\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train, y_train, epochs = 4, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr = 1e-4) ##default is 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2180 - accuracy: 0.9242 - val_loss: 0.3094 - val_accuracy: 0.8896\n",
      "Epoch 2/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2052 - accuracy: 0.9265 - val_loss: 0.3175 - val_accuracy: 0.8860\n",
      "Epoch 3/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1971 - accuracy: 0.9300 - val_loss: 0.3120 - val_accuracy: 0.8916\n",
      "Epoch 4/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1926 - accuracy: 0.9309 - val_loss: 0.3140 - val_accuracy: 0.8900\n",
      "Epoch 5/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1883 - accuracy: 0.9319 - val_loss: 0.3155 - val_accuracy: 0.8914\n",
      "Epoch 6/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1845 - accuracy: 0.9336 - val_loss: 0.3188 - val_accuracy: 0.8886\n",
      "Epoch 7/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1797 - accuracy: 0.9355 - val_loss: 0.3126 - val_accuracy: 0.8908\n",
      "Epoch 8/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1757 - accuracy: 0.9364 - val_loss: 0.3261 - val_accuracy: 0.8884\n",
      "Epoch 9/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1735 - accuracy: 0.9384 - val_loss: 0.3150 - val_accuracy: 0.8924\n",
      "Epoch 10/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1662 - accuracy: 0.9400 - val_loss: 0.3283 - val_accuracy: 0.8876\n",
      "Epoch 11/16\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.1660 - accuracy: 0.9390 - val_loss: 0.3410 - val_accuracy: 0.8890\n",
      "Epoch 12/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1628 - accuracy: 0.9411 - val_loss: 0.3233 - val_accuracy: 0.8920\n",
      "Epoch 13/16\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.1563 - accuracy: 0.9436 - val_loss: 0.3358 - val_accuracy: 0.8930\n",
      "Epoch 14/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1576 - accuracy: 0.9430 - val_loss: 0.3295 - val_accuracy: 0.8890\n",
      "Epoch 15/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1543 - accuracy: 0.9449 - val_loss: 0.3263 - val_accuracy: 0.8934\n",
      "Epoch 16/16\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1510 - accuracy: 0.9455 - val_loss: 0.3371 - val_accuracy: 0.8870\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train, y_train, epochs = 16, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3562 - accuracy: 0.8879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35621777176856995, 0.8878999948501587]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we only removed the outer layer and replace with a similar layer so the perofrmance stays the same. \n",
    "\n",
    "transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining\n",
    "\n",
    "if we cannot find a good amount of training data or a pretrained model we can try using unsupervised pretraining. it's cheaper to grather a lot of training data and then train one unsupervised model like autoencoder or a genrative adversial network. then we can reuse the lower layers of the autoencoder or the lower layers of GAN discriminator add the output layer for the task on top, and fine tune the final network usign supervised learning.\n",
    "\n",
    "#### Pretraining on an Auxiliary Task\n",
    "\n",
    "if we do not have a lot of trainign data, last option is to train a neural network on an auxillary task for which we can easily obtain or generate labeled training data. then reuse the lower layers of the network for the actual task. \n",
    "\n",
    "for nlp task, we can download a corpus of millions of text document and automatically generate labeled data from it.\n",
    "\n",
    "\n",
    "### Faster Optimizers\n",
    "\n",
    "we can speed up the training in 4 ways:\n",
    "- applyign a good initalization strategy\n",
    "- good activation function\n",
    "- using batch normalization\n",
    "- reusing the parts of a pretrained network\n",
    "\n",
    "we can also use a faster optmizer then the regular gradient descent. \n",
    "\n",
    "__Momentum Optimzation__\n",
    "\n",
    "regular gd takes regular steps down the slop, momentum optimizer speeds up on nearing the down slope. Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate η), and it updates the weights by adding this momentum vector. In other words, the gradient is used for acceleration, not for speed.  To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter β, called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.\n",
    "\n",
    "implementing the momentum in scikit\n",
    "\n",
    "`optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)`\n",
    "\n",
    "__Nesterov Accelerated Gradient__\n",
    "\n",
    "it works same as momentum, but uses the value of a little farther away to define the momentum. This small tweak works because in general the momentum vector will be pointing in the right direction. NAG is generally faster than regular momentum optimization\n",
    "\n",
    "`optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)`\n",
    "\n",
    "__AdaGrad__\n",
    "\n",
    "AG achieves a correction in direction more towrds the global minimum by scaling down the gradient vector along the steepest dimensions. In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum.\n",
    "\n",
    "AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum.\n",
    "\n",
    "\n",
    "__RMSProp__\n",
    "\n",
    "RMSProp fixes the problem of AdaGrad of slowing bit too fast and never converging to global optima. it this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step. The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not\n",
    "need to tune it at all.\n",
    "\n",
    "`optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)`\n",
    "\n",
    "__Adam and Nadam Optimization__\n",
    "\n",
    "Adam stands for adaptive moment estimation, combines the ideas of momentum optimization and RMSProp, like momentum optimization. \n",
    "\n",
    "\n",
    "- AdaMax\n",
    "- Nadam\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-ml-book",
   "language": "python",
   "name": "hands-on-ml-book"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
